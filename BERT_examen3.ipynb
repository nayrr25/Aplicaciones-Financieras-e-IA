{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nayrr25/Aplicaciones-Financieras-e-IA/blob/main/BERT_examen3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JeFdEXRfrOyw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca6rYUxNi_MO"
      },
      "source": [
        "# Fase 1: Importar las dependencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76HfPILdC5lD"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1h4YVFfDd1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21af97ef-02e5-44ac-abf4-47c099ee9c9c"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.10/dist-packages (0.14.9)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.10/dist-packages (from bert-for-tf2) (0.10.2)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.66.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMqTwu9jENrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60cef61c-09b3-40f5-e8a6-c7e33af93584"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0_xu0I3jFP9"
      },
      "source": [
        "# Fase 2: Pre procesado de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v60JTFKojIq5"
      },
      "source": [
        "## Carga de los ficheros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTtO7NkPjKUd"
      },
      "source": [
        "Importamos los ficheros desde nuestro Google Drive personal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRCxQui8Gqi_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "389bc9b1-e209-4fc0-aa32-b172138d3a9f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6iT5nxDHLRz"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Asegúrate de usar la ruta correcta a tu archivo\n",
        "file_path = '/content/drive/MyDrive/AplicacionesFinancieras/DataAnalyst.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X= df[['Job Title', 'Job Description','Sector']]\n",
        "y= df[['Salary Estimate']]"
      ],
      "metadata": {
        "id": "v3xQ59OVI2ns"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKnCVewUIBkc"
      },
      "source": [],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWWUo_XVeqoG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "16a411e4-25ae-4a99-c612-ed94707c40f9"
      },
      "source": [
        "X.head(6)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           Job Title  \\\n",
              "0  Data Analyst, Center on Immigration and Justic...   \n",
              "1                               Quality Data Analyst   \n",
              "2  Senior Data Analyst, Insights & Analytics Team...   \n",
              "3                                       Data Analyst   \n",
              "4                             Reporting Data Analyst   \n",
              "5                                       Data Analyst   \n",
              "\n",
              "                                     Job Description  \\\n",
              "0  Are you eager to roll up your sleeves and harn...   \n",
              "1  Overview\\n\\nProvides analytical and technical ...   \n",
              "2  We’re looking for a Senior Data Analyst who ha...   \n",
              "3  Requisition NumberRR-0001939\\nRemote:Yes\\nWe c...   \n",
              "4  ABOUT FANDUEL GROUP\\n\\nFanDuel Group is a worl...   \n",
              "5  About Cubist\\nCubist Systematic Strategies is ...   \n",
              "\n",
              "                             Sector  \n",
              "0                        Non-Profit  \n",
              "1                       Health Care  \n",
              "2            Information Technology  \n",
              "3            Information Technology  \n",
              "4  Arts, Entertainment & Recreation  \n",
              "5                           Finance  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8fcbf4d0-367d-4d5f-94e0-2c065235ea10\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Job Title</th>\n",
              "      <th>Job Description</th>\n",
              "      <th>Sector</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Data Analyst, Center on Immigration and Justic...</td>\n",
              "      <td>Are you eager to roll up your sleeves and harn...</td>\n",
              "      <td>Non-Profit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Quality Data Analyst</td>\n",
              "      <td>Overview\\n\\nProvides analytical and technical ...</td>\n",
              "      <td>Health Care</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Senior Data Analyst, Insights &amp; Analytics Team...</td>\n",
              "      <td>We’re looking for a Senior Data Analyst who ha...</td>\n",
              "      <td>Information Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>Requisition NumberRR-0001939\\nRemote:Yes\\nWe c...</td>\n",
              "      <td>Information Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reporting Data Analyst</td>\n",
              "      <td>ABOUT FANDUEL GROUP\\n\\nFanDuel Group is a worl...</td>\n",
              "      <td>Arts, Entertainment &amp; Recreation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>About Cubist\\nCubist Systematic Strategies is ...</td>\n",
              "      <td>Finance</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8fcbf4d0-367d-4d5f-94e0-2c065235ea10')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8fcbf4d0-367d-4d5f-94e0-2c065235ea10 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8fcbf4d0-367d-4d5f-94e0-2c065235ea10');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-21c03c80-6a9e-475b-9b27-249246e2409e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-21c03c80-6a9e-475b-9b27-249246e2409e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-21c03c80-6a9e-475b-9b27-249246e2409e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X",
              "summary": "{\n  \"name\": \"X\",\n  \"rows\": 262,\n  \"fields\": [\n    {\n      \"column\": \"Job Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 169,\n        \"samples\": [\n          \"Big Data Programmer Analyst\",\n          \"Market Data Reporting Analyst\",\n          \"Data Analyst, Enterprise Analytics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Job Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 262,\n        \"samples\": [\n          \"Requisition no: 507136\\nWork type: Full Time\\nLocation: Medical Center\\nSchool/Department: Medicine\\nGrade: Grade 103\\nCategories: Research (Lab and Non-Lab)\\nJob Type: Officer of Administration\\nBargaining Unit: N/A\\nRegular/Temporary: Regular\\nEnd Date if Temporary: N/A\\nHours Per Week: 35\\nSalary Range: Commensurate with experience\\nPosition Summary\\n\\nWe are an innovative research entity in the Division of Cardiology looking for a highly organized, detail-oriented and enthusiastic candidate to support the patient-oriented research portfolio of the Center. The Data Analyst will conduct data management activities and analysis for clinical research projects within the Center. The Data Analyst will work in conjunction with the Principal Investigator, other study investigators, and staff to coordinate data management and analysis activities.\\n\\nResponsibilities\\n\\nResponsibilities include:\\nDevelopment of data collection and data entry systems\\nRunning regular data quality assurance checks\\nOrganizing and merging research data from multiple sources\\nPreparing summary statistics, tables and figures for reports, presentations and publications\\nAssisting with the interpretation and dissemination of findings in peer-reviewed journals\\nCoordinating with research staff to ensure the accuracy and efficiency of the data collection and data entry tools and processes\\nPerforming other responsibilities as assigned and requested\\nMinimum Qualifications\\nRequires a bachelor's degree or equivalent in education, training and experience, plus two years of related experience.\\nPreferred Qualifications\\nExperience with database management (including REDCAP) preferred.\\nExperience with FileMaker Pro.\\nOther Requirements\\nExperience working with large datasets, statistical analysis, and strong working knowledge of advanced SAS and SPSS (or other statistical/database software or other programming language) required.\\nWell-organized, proactive, extremely organized and detail-oriented with excellent time management skills.\\nAble to balance working independently and within a multidisciplinary team\\nExcellent interpersonal, and communication skills.\\nAble to multitask in a diverse and demanding environment with frequently shifting priorities.\\nAble to demonstrate flexibility in workload/work hours to meet critical deadlines.\\nAbility to prioritize multiple projects to ensure the completion of essential tasks.\\nMust successfully complete systems training requirements.\\nEqual Opportunity Employer / Disability / Veteran\\n\\nColumbia University is committed to the hiring of qualified local residents.\\n\\nApplications open: Mar 18 2020 Eastern Daylight Time Applications close:\\n\\nBack Apply Share\",\n          \"Job Description\\n\\nJob ID#:\\n2359\\n\\nJob Category:\\nBehavioral Health (Promesa)\\n\\nPosition Type:\\nFull Time\\n\\nDetails:\\n\\n\\nAcacia Network, the leading Latino integrated care nonprofit in the nation, offers the community, from children to seniors, a pathway to behavioral and primary healthcare, housing, and empowerment. We are visionary leaders transforming the triple aim of high quality, great experience at a lower cost. Acacia champions a collaborative environment to deliver vital health, housing and community building services, work we have been doing since 1969. By hiring talented individuals like you, we\\u2019ve been able to expand quickly, with offices in Albany, Buffalo, Syracuse, Orlando, Tennessee, Maryland and Puerto Rico.\\n\\nThe Data Analyst is responsible for entering information into the Nextgen EHR, importing data results for analysis, analyzing data trends, completing QA/QI to improve program performance and outcomes based on trends and will be a member of the integrated treatment team. Under supervision of the Program Director, the requirements listed below represent the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities as defined by the ADA to perform the essential functions of the job. The functions below outline the general responsibilities associated with this position. Each of these functions is considered critical to effective department operations and our Mission. It is expected that all staff in this position will have satisfactory attendance and will perform these functions as assigned in a competent, cooperative, and timely manner. The Job Description will be reviewed periodically for accuracy and/or adjusted according to business necessity.\\n\\n\\nKEY ESSENTIAL FUNCTIONS:\\nWill enter all required data into Nextgen and run queries in SQL at the back end\\nWill run daily, weekly, monthly, and quarterly reports as per program needs and performance indicators.\\nGenerates quality indicator reports from Nextgen database as requested and assigned by supervisor.\\nMaintains Data spreadsheet up-to-date by keeping all required data indicators as per program regulations by mapping from front end to back end and validating it.\\nCoordinates and maintains record of consumer participation in the program by conducting surveys, activities and integrating consumers to participate in activities as per program regulations.\\nGenerates and inputs discharge and re-entry information for consumers on the Nextgen database as per program requirements and assigned by supervisor.\\nParticipates in integrated team meetings, collects data information as per program requirements and maintain accuracy in data spreadsheet.\\nComplete quality improvement projects based on data trends.\\nAct as curator of new datasets, documenting and performing quality checks.\\nCompletes all required training as per program regulations for entering consumer data and as assigned by supervisor.\\nCompletes office duties as needed/required by Supervisor.\\nCompletes additional tasks as assigned by Supervisor.\\n\\nREQUIREMENTS:\\nCollege degree preferred\\nKnowledge of the Nextgen database and proficiency in SQL\\nExperience or desire to work with people who have a mental illness\\nPositive attitude and professional demeanor\\nA self starter\\nAbility to complete work independently as well as in collaboration with team members\\nMust be team oriented with a willingness to be flexible and helpful.\\nExcellent computer skills including, Microsoft Windows, Excel, PowerPoint, and electronic communications tools: internet and email\\nAbility to communicate effectively orally and in writing\\nHighly organized, motivated self-starter\\nExcellent time management skills.\\nAcacia Network is an equal opportunity employer*\\n\\nJob Requirements\\n\\n\\nAlready have an account? Log in here\",\n          \"Data Analyst Assistant\\n\\nCalling data-loving entrepreneurs! Teus Health (www.teushealth.com), a healthcare policy and analytics firm, is growing and the owner, Dr. Tia Goss Sawhney, needs to grow her team. The work may be part- or full-time. Covid conditions permitting, the work is on-site at Teus Health\\u2019s downtown Newark office.\\n\\nDr. Sawhney needs your skills for:\\n\\n\\u00b7 Healthcare data analysis in SQL, Python, and/or other data analytic tools\\n\\n\\u00b7 Building and maintaining Excel models, requiring advanced Excel and VBA skills\\n\\n\\u00b7 Supporting and improving the firm\\u2019s IT environment and the security thereof\\n\\n\\u00b7 Preparing proposals\\n\\n\\u00b7 General business functions, including accounting (Quickbooks) and developing and implementing written policies and procedures\\n\\nWhile knowledge of healthcare is very much appreciated, the number one hiring criteria is that you be an experienced data-user and a fast-learning person who is able and willing to jump into ill-defined tasks and get them done \\u2013 well.\\n\\nSubmit a resume and a succinct cover letter. Within the cover letter, describe what will make you a great contributor to Teus Health\\u2019s growth. While you don\\u2019t have to have all the above skills, you need to sell your ability to deliver excellence. What excellence have you delivered in the past? The cover letter is also a writing sample, so write it without help from others.\\n\\nDr. Sawhney is committed to building a Newark, NJ-centered business. Therefore, preference will be given to candidates who live in Newark or nearby. You may be a student and if this work helps you satisfy degree requirements, Dr. Sawhney (an adjunct professor at NYU) will assist with the paperwork. Curricular practical training (CPT) sponsorship is possible for an exceptional candidate on an F-1 visa, but only if approval can be expedited.\\n\\nDr. Sawhney does not discriminate based on race, creed, color, national origin, ancestry, marital status, gender identity or expression, affectional or sexual orientation, or sex \\u2013 nor does she welcome those that do.\\n\\nJob Types: Full-time, Part-time, Temporary\\n\\nPay: $25.00 per hour\\n\\nSchedule:\\n\\nMonday to Friday\\nCOVID-19 considerations:\\nOnly Dr. Sawhney will be in the office with you with separate rooms and workspaces. She takes sanitation, mask, and handwashing protocols seriously upon any exit or entry of the building, and practices social distancing outside of work.\\n\\nWork Location:\\nOne location\\nTypical start time:\\n9AM\\nTypical end time:\\n5PM\\nThis Job Is Ideal for Someone Who Is:\\nDependable -- more reliable than spontaneous\\nAdaptable/flexible -- enjoys doing work that requires frequent shifts in direction\\nDetail-oriented -- would rather focus on the details of work than the bigger picture\\nAutonomous/Independent -- enjoys working with little direction\\nThis Company Describes Its Culture as:\\nDetail-oriented -- quality and precision-focused\\nOutcome-oriented -- results-focused with strong performance culture\\nStable -- traditional, stable, strong processes\\nCompany's website:\\nteushealth.com\\nWork Remotely:\\nNo\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sector\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"Non-Profit\",\n          \"Health Care\",\n          \"Restaurants, Bars & Food Services\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Quzx5tnjUtl"
      },
      "source": [
        "## Preprocesar la información"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8hlexmRjXIS"
      },
      "source": [
        "### Limpieza"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### convertir el df en dataframe\n"
      ],
      "metadata": {
        "id": "HdThE4iqtAEw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBSUDL-UP-W_"
      },
      "source": [
        "df = pd.DataFrame(df)\n",
        "\n",
        "# Limpiar el campo de estimación salarial\n",
        "df[\"Salary Estimate\"] = df[\"Salary Estimate\"].str.replace(r\" \\(Glassdoor est.\\)\", \"\", regex=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de normalización y conversión en variable numérica para la predicción\n",
        "df[\"Salary Estimate\"] = df[\"Salary Estimate\"].str.replace('K', '000')\n"
      ],
      "metadata": {
        "id": "LQYjlxloXAXI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# conversión de X en data_clean, sumando cada uno de los vectores\n",
        "data_clean= df['Job Title']+ \" \" + df['Job Description']+ \" \" + df['Sector']"
      ],
      "metadata": {
        "id": "M9-uOIQVLDYo"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"data_clean\"]= data_clean"
      ],
      "metadata": {
        "id": "yo9r-GP9te4h"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jiMaQsLWiTS"
      },
      "source": [
        "\n",
        "# Convertir rangos a valores numéricos y calcular el promedio\n",
        "df['min_salary'] = df['Salary Estimate'].str.split('-').str[0].str.replace('K', '').str.replace('$', '').astype(int) * 1000\n",
        "df['max_salary'] = df['Salary Estimate'].str.split('-').str[1].str.replace('K', '').str.replace('$', '').astype(int) * 1000\n",
        "df['avg_salary'] = (df['min_salary'] + df['max_salary']) / 2\n",
        "\n",
        "# Mostrar las primeras filas después de la conversión para verificar\n",
        "#print(\"Primeras filas después de la conversión:\", df[['Salary Estimate', 'min_salary', 'max_salary', 'avg_salary']].head())\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['avg_salary']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XynuwNp2xrs9",
        "outputId": "21fa7cef-b5a2-43cd-bc77-5bcb006fe621"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       51500000.0\n",
              "1       51500000.0\n",
              "2       51500000.0\n",
              "3       51500000.0\n",
              "4       51500000.0\n",
              "          ...     \n",
              "257    100000000.0\n",
              "258    100000000.0\n",
              "259    100000000.0\n",
              "260    100000000.0\n",
              "261    100000000.0\n",
              "Name: avg_salary, Length: 262, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# de las tres posibilidades, uitilizaré el promedio"
      ],
      "metadata": {
        "id": "0j79HJJIttTD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_labels = df[\"avg_salary\"].values\n",
        "# Convert to a pandas Series\n",
        "data_labels = pd.Series(data_labels)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "print(data_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot75zYH_yXhr",
        "outputId": "9b2c1fcc-c8d9-4a30-d8ff-80c5fe1e95ae"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      51500000.00\n",
            "1      51500000.00\n",
            "2      51500000.00\n",
            "3      51500000.00\n",
            "4      51500000.00\n",
            "          ...     \n",
            "257   100000000.00\n",
            "258   100000000.00\n",
            "259   100000000.00\n",
            "260   100000000.00\n",
            "261   100000000.00\n",
            "Length: 262, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eh7sIquja5t"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K59PriX4jgBV"
      },
      "source": [
        "Necesitaremos crear una capa BERT para tener acceso a los meta datos para el tokenizador (como el tamaño del vocabulario)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 1: Importar las bibliotecas necesarias\n",
        "Asegúrate de que tienes las bibliotecas necesarias importadas para ejecutar el código, como tensorflow_hub y bert-for-tf2:"
      ],
      "metadata": {
        "id": "BOz3ghSrza4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "#!pip install bert-for-tf2\n",
        "import bert\n"
      ],
      "metadata": {
        "id": "P05hnDYZzdnD"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 2: Cargar la Capa BERT desde TensorFlow Hub\n",
        "El código carga un modelo BERT (bert_en_uncased_L-12_H-768_A-12) como una capa de Keras de TensorFlow Hub, que no será entrenable (trainable=False). Esto es útil para tareas que solo requieren la representación de BERT sin ajustar sus pesos:"
      ],
      "metadata": {
        "id": "0r5NZ-10zkOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)"
      ],
      "metadata": {
        "id": "D47plSVYzdj7"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 3: Preparar el Tokenizer\n",
        "Se prepara el tokenizer utilizando la información del modelo cargado:\n",
        "\n",
        "Archivo de Vocabulario: vocab_file se obtiene directamente del modelo BERT cargado, que contiene el vocabulario usado durante el entrenamiento del modelo.\n",
        "Case Sensitivity: do_lower_case indica si el modelo fue entrenado con textos en minúsculas, lo cual es importante para saber cómo procesar el texto de entrada."
      ],
      "metadata": {
        "id": "GFj7rEYkzrbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "metadata": {
        "id": "hEMVFDvpzdhE"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 4: Utilizar el Tokenizer\n",
        "Una vez que tienes el tokenizer configurado, puedes usarlo para tokenizar tus textos. Por ejemplo:"
      ],
      "metadata": {
        "id": "aSNEkt9-zzqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Here is an example sentence to tokenize using BERT tokenizer.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PcXW-pSzdec",
        "outputId": "7262e668-6ab9-4cd4-efbc-cee86c69df9f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['here', 'is', 'an', 'example', 'sentence', 'to', 'token', '##ize', 'using', 'bert', 'token', '##izer', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 5: Aplicar el Tokenizer\n",
        "Una vez que tienes la columna data_clean lista, puedes aplicar el tokenizer a cada entrada de esta columna. Usaremos una comprensión de lista para aplicar el tokenizer a cada fila del DataFrame."
      ],
      "metadata": {
        "id": "USPb_2OQ0mMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que 'tokenizer' ya está inicializado como mostraste anteriormente\n",
        "df['tokens'] = df['data_clean'].apply(lambda x: tokenizer.tokenize(x))"
      ],
      "metadata": {
        "id": "BzJI9sI5zdb2"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tokens']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4kGf3gC1eP3",
        "outputId": "ec69829e-2a5e-4072-b91a-87403b03bfe0"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      [data, analyst, ,, center, on, immigration, an...\n",
              "1      [quality, data, analyst, overview, provides, a...\n",
              "2      [senior, data, analyst, ,, insights, &, analyt...\n",
              "3      [data, analyst, re, ##qui, ##sit, ##ion, numbe...\n",
              "4      [reporting, data, analyst, about, fan, ##due, ...\n",
              "                             ...                        \n",
              "257    [data, analyst, -, qc, nes, ##co, resource, is...\n",
              "258    [people, operations, &, data, analyst, job, de...\n",
              "259    [lead, data, analyst, (, product, ), a, bit, a...\n",
              "260    [data, analyst, -, iii, direct, client, requir...\n",
              "261    [sql, data, analyst, job, title, :, senior, sq...\n",
              "Name: tokens, Length: 262, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código añadirá una nueva columna al DataFrame llamada 'tokens', donde cada entrada es la lista de tokens resultantes de aplicar el tokenizer BERT a la cadena correspondiente en 'data_clean'.\n",
        "\n",
        "### Paso 6: Verificación\n",
        "Es una buena práctica verificar que el proceso ha funcionado como se esperaba. Puedes inspeccionar las primeras filas del DataFrame para asegurarte de que los tokens se han generado correctamente:"
      ],
      "metadata": {
        "id": "cbcgYDgq00oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['data_clean', 'tokens']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4QVRqqJzdYO",
        "outputId": "68eebd3e-67cd-4c7c-a2e8-9afc9f308558"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          data_clean  \\\n",
            "0  Data Analyst, Center on Immigration and Justic...   \n",
            "1  Quality Data Analyst Overview\\n\\nProvides anal...   \n",
            "2  Senior Data Analyst, Insights & Analytics Team...   \n",
            "3  Data Analyst Requisition NumberRR-0001939\\nRem...   \n",
            "4  Reporting Data Analyst ABOUT FANDUEL GROUP\\n\\n...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [data, analyst, ,, center, on, immigration, an...  \n",
            "1  [quality, data, analyst, overview, provides, a...  \n",
            "2  [senior, data, analyst, ,, insights, &, analyt...  \n",
            "3  [data, analyst, re, ##qui, ##sit, ##ion, numbe...  \n",
            "4  [reporting, data, analyst, about, fan, ##due, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 7: tokenizar las palabras"
      ],
      "metadata": {
        "id": "hMmRG08G2gKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función encode_sentence que mencionas está diseñada para convertir un texto de entrada en una lista de identificadores numéricos (IDs) correspondientes a los tokens en el vocabulario de BERT. Este proceso es una parte crucial en la preparación de datos para modelos basados en BERT, ya que estos modelos no trabajan directamente con texto plano sino con tokens que son convertidos en IDs numéricos."
      ],
      "metadata": {
        "id": "KmfERaZX2aWV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LggMv7k7Z3Ij"
      },
      "source": [
        "def encode_sentence(sent):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 8: Conversión de Tokens a IDs\n",
        "Una vez que el texto ha sido dividido en tokens, el siguiente paso es convertir cada token en su correspondiente ID numérico:"
      ],
      "metadata": {
        "id": "GIZOXYBi2tjL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGfTo5uIa2is"
      },
      "source": [
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-4oGSu5jxUi"
      },
      "source": [
        "# Creación del data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLg0Z7QOj_YZ"
      },
      "source": [
        "Crearemos padded batches (por lo que rellenamos las frases para cada lote de forma independiente), de esta forma añadimos el mínimo número de tokens de padding posible. Para eso, ordenamos las frases por longitud, aplicamos padded_batches y luego las mezclamos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS_f6gWsLfLM"
      },
      "source": [
        "data_with_len = [[sent, data_labels[i], len(sent)]\n",
        "                 for i, sent in enumerate(data_inputs)]\n",
        "random.shuffle(data_with_len)\n",
        "data_with_len.sort(key=lambda x: x[2])\n",
        "sorted_all = [(sent_lab[0], sent_lab[1])\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 7]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry0uJJg8lSQR"
      },
      "source": [
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
        "                                             output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzHAhlfTlrcj"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrPqJeYpmfcv"
      },
      "source": [
        "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10\n",
        "all_batched.shuffle(NB_BATCHES)\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
        "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU-YHkMVVqVY",
        "outputId": "7a759c65-7401-4dd0-e541-ec2c7ceaacc9"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws9aABiFVtVW",
        "outputId": "2c1771f7-2c43-4f4d-de33-a7e8cc626c4c"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_SkipDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxONsFVHkFLU"
      },
      "source": [
        "# Fase 3: Construcción del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6DD3k3qPLDQ"
      },
      "source": [
        "#DCNN que hereda de tf.keras.Model de TensorFlow Keras API, diseñada específicamente para trabajar con datos textuales,\n",
        "#probablemente para tareas como clasificación de texto.\n",
        "class DCNN(tf.keras.Model):\n",
        "  #El método __init__ inicializa una nueva instancia de DCNN. Aquí es donde configuras todas las capas y parámetros que necesitará tu modelo:\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 emb_dim=128,\n",
        "                 nb_filters=50,\n",
        "                 FFN_units=512,\n",
        "                 nb_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"dcnn\"):\n",
        "        super(DCNN, self).__init__(name=name)\n",
        "        #Capas del Modelo: Después del llamado a super(), se inicializan varias capas que componen el modelo:\n",
        "        self.embedding = layers.Embedding(vocab_size,\n",
        "                                          emb_dim)\n",
        "        #Una capa de embedding que transforma los índices de palabras en vectores densos de tamaño emb_dim.\n",
        "        #Tres capas convolucionales 1D para capturar características locales dentro de ventanas de tamaño 2, 3, y 4\n",
        "        #(bigramas, trigramas, y fourgramas). Utilizan ReLU como función de activación.\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
        "                                    kernel_size=2,\n",
        "                                    padding=\"valid\",\n",
        "                                    activation=\"relu\")\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
        "                                     kernel_size=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
        "                                      kernel_size=4,\n",
        "                                      padding=\"valid\",\n",
        "                                      activation=\"relu\")\n",
        "        #Una capa de pooling global que obtiene el máximo de cada feature map, reduciendo la dimensión temporal.\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        #Una capa densa para clasificación, seguida de una capa de dropout para regularización. La última capa densa\n",
        "        #varía en función de si el problema es de clasificación binaria (sigmoide) o multiclase (softmax).\n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                           activation=\"softmax\")\n",
        "        #Este método define el flujo de datos a través del modelo (forward pass).\n",
        "        #Aquí es donde se combinan todas las capas definidas en el constructor:\n",
        "        #Cada sentencia en este bloque toma los inputs, los pasa a través de las capas de embedding y convolucionales, aplica pooling,\n",
        "        # y luego concatena los resultados. Finalmente, pasa por capas densas adicionales para realizar la clasificación.\n",
        "    def call(self, inputs, training):\n",
        "        x = self.embedding(inputs)\n",
        "        x_1 = self.bigram(x) # (batch_size, nb_filters, seq_len-1)\n",
        "        x_1 = self.pool(x_1) # (batch_size, nb_filters)\n",
        "        x_2 = self.trigram(x) # (batch_size, nb_filters, seq_len-2)\n",
        "        x_2 = self.pool(x_2) # (batch_size, nb_filters)\n",
        "        x_3 = self.fourgram(x) # (batch_size, nb_filters, seq_len-3)\n",
        "        x_3 = self.pool(x_3) # (batch_size, nb_filters)\n",
        "\n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
        "        merged = self.dense_1(merged)\n",
        "        merged = self.dropout(merged, training)\n",
        "        output = self.last_dense(merged)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSix1l4jkIxp"
      },
      "source": [
        "# Fase 4: Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhfUFvWEPOIf"
      },
      "source": [
        "VOCAB_SIZE = len(tokenizer.vocab)\n",
        "EMB_DIM = 200\n",
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "NB_EPOCHS = 5"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMtdiWmwv6rD"
      },
      "source": [
        "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
        "            emb_dim=EMB_DIM,\n",
        "            nb_filters=NB_FILTERS,\n",
        "            FFN_units=FFN_UNITS,\n",
        "            nb_classes=NB_CLASSES,\n",
        "            dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6apbd7FrwPYo"
      },
      "source": [
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78cceSGCw1XC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d05a085-8510-4cb0-cdde-b9c1d74f247d"
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Último checkpoint restaurado!!\")"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Último checkpoint restaurado!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YIF5trzx7RA"
      },
      "source": [
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        ckpt_manager.save()\n",
        "        print(\"Checkpoint guardado en {}.\".format(checkpoint_path))"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrT8oWZzQNmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac59acbe-fe23-4233-80c6-104f70e0cbff"
      },
      "source": [
        "Dcnn.fit(train_dataset,\n",
        "         epochs=NB_EPOCHS,\n",
        "         callbacks=[MyCustomCallback()])"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "      9/Unknown - 4s 112ms/step - loss: -81288329428992.0000 - accuracy: 0.0000e+00Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/.\n",
            "9/9 [==============================] - 4s 154ms/step - loss: -81288329428992.0000 - accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "8/9 [=========================>....] - ETA: 0s - loss: -94214436159488.0000 - accuracy: 0.0000e+00Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/.\n",
            "9/9 [==============================] - 1s 130ms/step - loss: -94747213430784.0000 - accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "8/9 [=========================>....] - ETA: 0s - loss: -109806232797184.0000 - accuracy: 0.0000e+00Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/.\n",
            "9/9 [==============================] - 0s 56ms/step - loss: -110243832922112.0000 - accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "8/9 [=========================>....] - ETA: 0s - loss: -125890038071296.0000 - accuracy: 0.0000e+00Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/.\n",
            "9/9 [==============================] - 1s 63ms/step - loss: -126402313584640.0000 - accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "8/9 [=========================>....] - ETA: 0s - loss: -144851312050176.0000 - accuracy: 0.0000e+00Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/.\n",
            "9/9 [==============================] - 1s 59ms/step - loss: -145554629722112.0000 - accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e2e7e719e40>"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IiDW919kQQK"
      },
      "source": [
        "# Fase 5: Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo: Si tienes 1000 ejemplos y un tamaño de lote de 32\n",
        "total_test_samples = 1000\n",
        "batch_size = 32\n",
        "\n",
        "steps_per_epoch = total_test_samples // batch_size\n",
        "if total_test_samples % batch_size != 0:\n",
        "    steps_per_epoch += 100  # Añadir un paso adicional si hay un remanente\n",
        "\n",
        "results = Dcnn.evaluate(test_dataset, steps=steps_per_epoch)\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jjJnCfKfEOf",
        "outputId": "c75476ab-9c2c-47fb-e2a4-780171841cd6"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 131 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r131/131 [==============================] - 0s 1ms/step\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU0FPb-Nv0YO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1548e700-21e7-4055-8a0f-1f5d7e7b2c0f"
      },
      "source": [
        "results"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17dbJ-bS_Izy"
      },
      "source": [
        "\n",
        "\n",
        "*   Training: 88.5%\n",
        "*   Testing: 84.6%\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-jrRvtl1xuk"
      },
      "source": [
        "def get_prediction(sentence):\n",
        "    tokens = encode_sentence(sentence)\n",
        "    inputs = tf.expand_dims(tokens, 0)\n",
        "\n",
        "    output = Dcnn(inputs, training=False)\n",
        "\n",
        "    sentiment = math.floor(output*2)\n",
        "\n",
        "    if sentiment == 0:\n",
        "        print(\"Salida del modelo: {}\\nSentimiento predicho: Negativo.\".format(\n",
        "            output))\n",
        "    elif sentiment == 1:\n",
        "        print(\"Salida del modelo: {}\\nSentimiento predicho: Positivo.\".format(\n",
        "            output))"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk8V2bdvwfCv"
      },
      "source": [
        "get_prediction(\"This movie was pretty interesting.\")"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIcwVVB7wFUM"
      },
      "source": [
        "get_prediction(\"I'd rather not do that again.\")"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qRnG7ojuNCBr"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regresión con BERT\n",
        "Preparación de Datos\n",
        "Primero, asegúrate de que tus datos están correctamente formateados y preparados. Esto incluiría:\n",
        "\n",
        "Limpiar y preparar las descripciones de empleo.\n",
        "Convertir los rangos salariales a un formato numérico si optas por regresión, o categorizarlos si prefieres clasificación.\n",
        "2. Modelo de Regresión con BERT\n",
        "Si los salarios son valores continuos y decides construir un modelo de regresión:\n",
        "\n",
        "a. Usar BERT como Extractor de Características\n",
        "Cargar BERT preentrenado y usarlo para transformar descripciones de empleo en embeddings. Estos embeddings actuarán como entradas para la capa de regresión."
      ],
      "metadata": {
        "id": "qjV3sLXm6wSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "encoded_inputs = tokenizer.batch_encode_plus(\n",
        "    data_inputs,\n",
        "    add_special_tokens=True,    # Añadir tokens especiales como [CLS] y [SEP]\n",
        "    max_length=128,             # Longitud máxima de secuencia\n",
        "    padding='max_length',       # Padear todas las secuencias a la longitud máxima especificada\n",
        "    truncation=True,            # Truncar las secuencias a la longitud máxima especificada\n",
        "    return_tensors='tf'         # Devolver los resultados como tensores de TensorFlow\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "6bv3Vh6vCaRf",
        "outputId": "995fd703-9ca0-4844-dfd3-b007a8156935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-144-be1a4364a746>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m encoded_inputs = tokenizer.batch_encode_plus(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdata_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;31m# Añadir tokens especiales como [CLS] y [SEP]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3104\u001b[0m         )\n\u001b[1;32m   3105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3106\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3107\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3108\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    799\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Suponiendo que estás usando 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Supongamos que 'data_inputs' es una lista de textos a tokenizar\n",
        "encoded_inputs = tokenizer.batch_encode_plus(\n",
        "    data_inputs,\n",
        "    add_special_tokens=True,    # Añadir tokens especiales (CLS, SEP)\n",
        "    max_length=128,             # Longitud máxima para padear/truncar\n",
        "    padding='max_length',       # Padear a la longitud máxima\n",
        "    truncation=True,            # Truncar a la longitud máxima\n",
        "    return_tensors='tf'         # Devolver tensores TensorFlow\n",
        ")\n",
        "\n",
        "# Extraer `input_ids`, `attention_masks` y `token_type_ids`\n",
        "input_ids = encoded_inputs['input_ids']\n",
        "attention_masks = encoded_inputs['attention_mask']\n",
        "token_type_ids = encoded_inputs['token_type_ids']\n",
        "\n"
      ],
      "metadata": {
        "id": "8BiK4yQi8kSn",
        "outputId": "991aef20-0167-4aa5-b864-f7da51ad40fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-143-f92c10d33a34>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Supongamos que 'data_inputs' es una lista de textos a tokenizar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m encoded_inputs = tokenizer.batch_encode_plus(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdata_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;31m# Añadir tokens especiales (CLS, SEP)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3104\u001b[0m         )\n\u001b[1;32m   3105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3106\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3107\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3108\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    799\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    input_ids,\n",
        "    attention_masks,\n",
        "    token_type_ids\n",
        "))\n",
        "\n",
        "test_dataset = test_dataset.map(\n",
        "    lambda x, y, z: (\n",
        "        {'input_word_ids': x, 'input_mask': y, 'segment_ids': z},\n",
        "    )\n",
        ")\n",
        "\n",
        "batch_size = 32  # Definir según tus necesidades\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "\n",
        "# Verificación opcional\n",
        "print(\"Input IDs:\", input_ids.shape)\n",
        "print(\"Attention Masks:\", attention_masks.shape)\n",
        "print(\"Token Type IDs:\", token_type_ids.shape)\n"
      ],
      "metadata": {
        "id": "kAnE4LviAhly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XaeQWpHHAhiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bV4KotA5AhXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids= data_inputs\n",
        "# Suponiendo que tienes `input_ids`, `attention_masks` y `token_type_ids` preparados\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((input_ids, attention_masks, token_type_ids))\n",
        "test_dataset = test_dataset.map(lambda x, y, z: ({'input_word_ids': x, 'input_mask': y, 'segment_ids': z},))\n",
        "test_dataset = test_dataset.batch(batch_size)  # Asegúrate de definir un batch_size adecuado\n"
      ],
      "metadata": {
        "id": "ChLawwlB7pd5",
        "outputId": "017cd1d3-ba5b-4284-edf0-ea05ec371086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'attention_masks' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-125-323966503f7e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdata_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Suponiendo que tienes `input_ids`, `attention_masks` y `token_type_ids` preparados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'input_word_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input_mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'segment_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Asegúrate de definir un batch_size adecuado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'attention_masks' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "\n",
        "# Carga BERT desde TensorFlow Hub\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=True)\n",
        "\n",
        "# Modelo para convertir texto en embeddings\n",
        "input_word_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "input_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"input_mask\")\n",
        "segment_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "dense_layer = tf.keras.layers.Dense(1, activation='linear')(pooled_output)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=dense_layer)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse', metrics=['mae'])\n"
      ],
      "metadata": {
        "id": "OOEowq7E66Hb"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Entrenamiento del Modelo\n",
        "Entrenar este modelo con tus datos (textos tokenizados y salarios numéricos).\n",
        "\n",
        "3. Modelo de Clasificación con BERT\n",
        "Si decides manejar los salarios en categorías (como rangos de salario):\n",
        "\n",
        "a. Capa de Clasificación sobre BERT\n",
        "Configurar BERT con una capa de clasificación encima, similar al modelo anterior, pero cambiando la función de activación y el número de unidades en la capa densa final para reflejar el número de clases (rangos salariales).\n",
        "\n"
      ],
      "metadata": {
        "id": "NToKXLR77Arm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10  # Supongamos que hay 10 rangos salariales diferentes\n",
        "dense_layer = tf.keras.layers.Dense(num_classes, activation='softmax')(pooled_output)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=dense_layer)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "GG3F6EGn6_Ek"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Entrenamiento y Evaluación\n",
        "Independientemente del enfoque, deberás entrenar y evaluar tu modelo utilizando tus datos preparados. Asegúrate de dividir tus datos en conjuntos de entrenamiento, validación y prueba para poder evaluar la efectividad de tu modelo adecuadamente.\n",
        "\n",
        "5. Consideraciones Adicionales\n",
        "Fine-tuning de BERT: Aunque puedes usar BERT solo para extraer características, permitir un fine-tuning limitado de BERT con tus datos específicos podría mejorar la performance.\n",
        "Longitud de los Textos: BERT maneja hasta 512 tokens; si tus descripciones son más largas, necesitarás estrategias para manejar esto, como truncar o dividir los textos.\n",
        "Preprocesamiento: Asegúrate de aplicar el mismo preprocesamiento de tokens y consideraciones especiales que BERT utiliza durante el entrenamiento.\n",
        "Al construir un modelo sobre BERT para este tipo de tarea, estás aprovechando uno de los modelos de lenguaje más avanzados disponibles, lo cual puede resultar en predicciones de alta calidad si se configura y entrena correctamente."
      ],
      "metadata": {
        "id": "b4eJZy0a7M8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Asumiendo que 'test_dataset' es tu conjunto de datos de prueba y está preparado adecuadamente\n",
        "predictions = model.predict(test_dataset)\n"
      ],
      "metadata": {
        "id": "xlBjWM3j7MBl",
        "outputId": "c3bc0644-b71d-4364-8e90-8e8f045da83b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_1\" expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None) dtype=int32>]\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-653ef59f4a39>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Asumiendo que 'test_dataset' es tu conjunto de datos de prueba y está preparado adecuadamente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_1\" expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None) dtype=int32>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i3-Ik5mO7YjN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}