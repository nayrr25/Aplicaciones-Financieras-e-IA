{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nayrr25/Aplicaciones-Financieras-e-IA/blob/main/BERT_examen3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JeFdEXRfrOyw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca6rYUxNi_MO"
      },
      "source": [
        "# Fase 1: Importar las dependencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76HfPILdC5lD"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1h4YVFfDd1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21af97ef-02e5-44ac-abf4-47c099ee9c9c"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.10/dist-packages (0.14.9)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.10/dist-packages (from bert-for-tf2) (0.10.2)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.66.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMqTwu9jENrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60cef61c-09b3-40f5-e8a6-c7e33af93584"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0_xu0I3jFP9"
      },
      "source": [
        "# Fase 2: Pre procesado de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v60JTFKojIq5"
      },
      "source": [
        "## Carga de los ficheros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTtO7NkPjKUd"
      },
      "source": [
        "Importamos los ficheros desde nuestro Google Drive personal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRCxQui8Gqi_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "389bc9b1-e209-4fc0-aa32-b172138d3a9f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6iT5nxDHLRz"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Asegúrate de usar la ruta correcta a tu archivo\n",
        "file_path = '/content/drive/MyDrive/AplicacionesFinancieras/DataAnalyst.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X= df[['Job Title', 'Job Description','Sector']]\n",
        "y= df[['Salary Estimate']]"
      ],
      "metadata": {
        "id": "v3xQ59OVI2ns"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKnCVewUIBkc"
      },
      "source": [],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWWUo_XVeqoG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "16a411e4-25ae-4a99-c612-ed94707c40f9"
      },
      "source": [
        "X.head(6)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           Job Title  \\\n",
              "0  Data Analyst, Center on Immigration and Justic...   \n",
              "1                               Quality Data Analyst   \n",
              "2  Senior Data Analyst, Insights & Analytics Team...   \n",
              "3                                       Data Analyst   \n",
              "4                             Reporting Data Analyst   \n",
              "5                                       Data Analyst   \n",
              "\n",
              "                                     Job Description  \\\n",
              "0  Are you eager to roll up your sleeves and harn...   \n",
              "1  Overview\\n\\nProvides analytical and technical ...   \n",
              "2  We’re looking for a Senior Data Analyst who ha...   \n",
              "3  Requisition NumberRR-0001939\\nRemote:Yes\\nWe c...   \n",
              "4  ABOUT FANDUEL GROUP\\n\\nFanDuel Group is a worl...   \n",
              "5  About Cubist\\nCubist Systematic Strategies is ...   \n",
              "\n",
              "                             Sector  \n",
              "0                        Non-Profit  \n",
              "1                       Health Care  \n",
              "2            Information Technology  \n",
              "3            Information Technology  \n",
              "4  Arts, Entertainment & Recreation  \n",
              "5                           Finance  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8fcbf4d0-367d-4d5f-94e0-2c065235ea10\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Job Title</th>\n",
              "      <th>Job Description</th>\n",
              "      <th>Sector</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Data Analyst, Center on Immigration and Justic...</td>\n",
              "      <td>Are you eager to roll up your sleeves and harn...</td>\n",
              "      <td>Non-Profit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Quality Data Analyst</td>\n",
              "      <td>Overview\\n\\nProvides analytical and technical ...</td>\n",
              "      <td>Health Care</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Senior Data Analyst, Insights &amp; Analytics Team...</td>\n",
              "      <td>We’re looking for a Senior Data Analyst who ha...</td>\n",
              "      <td>Information Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>Requisition NumberRR-0001939\\nRemote:Yes\\nWe c...</td>\n",
              "      <td>Information Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reporting Data Analyst</td>\n",
              "      <td>ABOUT FANDUEL GROUP\\n\\nFanDuel Group is a worl...</td>\n",
              "      <td>Arts, Entertainment &amp; Recreation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>About Cubist\\nCubist Systematic Strategies is ...</td>\n",
              "      <td>Finance</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8fcbf4d0-367d-4d5f-94e0-2c065235ea10')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8fcbf4d0-367d-4d5f-94e0-2c065235ea10 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8fcbf4d0-367d-4d5f-94e0-2c065235ea10');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-21c03c80-6a9e-475b-9b27-249246e2409e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-21c03c80-6a9e-475b-9b27-249246e2409e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-21c03c80-6a9e-475b-9b27-249246e2409e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X",
              "summary": "{\n  \"name\": \"X\",\n  \"rows\": 262,\n  \"fields\": [\n    {\n      \"column\": \"Job Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 169,\n        \"samples\": [\n          \"Big Data Programmer Analyst\",\n          \"Market Data Reporting Analyst\",\n          \"Data Analyst, Enterprise Analytics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Job Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 262,\n        \"samples\": [\n          \"Requisition no: 507136\\nWork type: Full Time\\nLocation: Medical Center\\nSchool/Department: Medicine\\nGrade: Grade 103\\nCategories: Research (Lab and Non-Lab)\\nJob Type: Officer of Administration\\nBargaining Unit: N/A\\nRegular/Temporary: Regular\\nEnd Date if Temporary: N/A\\nHours Per Week: 35\\nSalary Range: Commensurate with experience\\nPosition Summary\\n\\nWe are an innovative research entity in the Division of Cardiology looking for a highly organized, detail-oriented and enthusiastic candidate to support the patient-oriented research portfolio of the Center. The Data Analyst will conduct data management activities and analysis for clinical research projects within the Center. The Data Analyst will work in conjunction with the Principal Investigator, other study investigators, and staff to coordinate data management and analysis activities.\\n\\nResponsibilities\\n\\nResponsibilities include:\\nDevelopment of data collection and data entry systems\\nRunning regular data quality assurance checks\\nOrganizing and merging research data from multiple sources\\nPreparing summary statistics, tables and figures for reports, presentations and publications\\nAssisting with the interpretation and dissemination of findings in peer-reviewed journals\\nCoordinating with research staff to ensure the accuracy and efficiency of the data collection and data entry tools and processes\\nPerforming other responsibilities as assigned and requested\\nMinimum Qualifications\\nRequires a bachelor's degree or equivalent in education, training and experience, plus two years of related experience.\\nPreferred Qualifications\\nExperience with database management (including REDCAP) preferred.\\nExperience with FileMaker Pro.\\nOther Requirements\\nExperience working with large datasets, statistical analysis, and strong working knowledge of advanced SAS and SPSS (or other statistical/database software or other programming language) required.\\nWell-organized, proactive, extremely organized and detail-oriented with excellent time management skills.\\nAble to balance working independently and within a multidisciplinary team\\nExcellent interpersonal, and communication skills.\\nAble to multitask in a diverse and demanding environment with frequently shifting priorities.\\nAble to demonstrate flexibility in workload/work hours to meet critical deadlines.\\nAbility to prioritize multiple projects to ensure the completion of essential tasks.\\nMust successfully complete systems training requirements.\\nEqual Opportunity Employer / Disability / Veteran\\n\\nColumbia University is committed to the hiring of qualified local residents.\\n\\nApplications open: Mar 18 2020 Eastern Daylight Time Applications close:\\n\\nBack Apply Share\",\n          \"Job Description\\n\\nJob ID#:\\n2359\\n\\nJob Category:\\nBehavioral Health (Promesa)\\n\\nPosition Type:\\nFull Time\\n\\nDetails:\\n\\n\\nAcacia Network, the leading Latino integrated care nonprofit in the nation, offers the community, from children to seniors, a pathway to behavioral and primary healthcare, housing, and empowerment. We are visionary leaders transforming the triple aim of high quality, great experience at a lower cost. Acacia champions a collaborative environment to deliver vital health, housing and community building services, work we have been doing since 1969. By hiring talented individuals like you, we\\u2019ve been able to expand quickly, with offices in Albany, Buffalo, Syracuse, Orlando, Tennessee, Maryland and Puerto Rico.\\n\\nThe Data Analyst is responsible for entering information into the Nextgen EHR, importing data results for analysis, analyzing data trends, completing QA/QI to improve program performance and outcomes based on trends and will be a member of the integrated treatment team. Under supervision of the Program Director, the requirements listed below represent the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities as defined by the ADA to perform the essential functions of the job. The functions below outline the general responsibilities associated with this position. Each of these functions is considered critical to effective department operations and our Mission. It is expected that all staff in this position will have satisfactory attendance and will perform these functions as assigned in a competent, cooperative, and timely manner. The Job Description will be reviewed periodically for accuracy and/or adjusted according to business necessity.\\n\\n\\nKEY ESSENTIAL FUNCTIONS:\\nWill enter all required data into Nextgen and run queries in SQL at the back end\\nWill run daily, weekly, monthly, and quarterly reports as per program needs and performance indicators.\\nGenerates quality indicator reports from Nextgen database as requested and assigned by supervisor.\\nMaintains Data spreadsheet up-to-date by keeping all required data indicators as per program regulations by mapping from front end to back end and validating it.\\nCoordinates and maintains record of consumer participation in the program by conducting surveys, activities and integrating consumers to participate in activities as per program regulations.\\nGenerates and inputs discharge and re-entry information for consumers on the Nextgen database as per program requirements and assigned by supervisor.\\nParticipates in integrated team meetings, collects data information as per program requirements and maintain accuracy in data spreadsheet.\\nComplete quality improvement projects based on data trends.\\nAct as curator of new datasets, documenting and performing quality checks.\\nCompletes all required training as per program regulations for entering consumer data and as assigned by supervisor.\\nCompletes office duties as needed/required by Supervisor.\\nCompletes additional tasks as assigned by Supervisor.\\n\\nREQUIREMENTS:\\nCollege degree preferred\\nKnowledge of the Nextgen database and proficiency in SQL\\nExperience or desire to work with people who have a mental illness\\nPositive attitude and professional demeanor\\nA self starter\\nAbility to complete work independently as well as in collaboration with team members\\nMust be team oriented with a willingness to be flexible and helpful.\\nExcellent computer skills including, Microsoft Windows, Excel, PowerPoint, and electronic communications tools: internet and email\\nAbility to communicate effectively orally and in writing\\nHighly organized, motivated self-starter\\nExcellent time management skills.\\nAcacia Network is an equal opportunity employer*\\n\\nJob Requirements\\n\\n\\nAlready have an account? Log in here\",\n          \"Data Analyst Assistant\\n\\nCalling data-loving entrepreneurs! Teus Health (www.teushealth.com), a healthcare policy and analytics firm, is growing and the owner, Dr. Tia Goss Sawhney, needs to grow her team. The work may be part- or full-time. Covid conditions permitting, the work is on-site at Teus Health\\u2019s downtown Newark office.\\n\\nDr. Sawhney needs your skills for:\\n\\n\\u00b7 Healthcare data analysis in SQL, Python, and/or other data analytic tools\\n\\n\\u00b7 Building and maintaining Excel models, requiring advanced Excel and VBA skills\\n\\n\\u00b7 Supporting and improving the firm\\u2019s IT environment and the security thereof\\n\\n\\u00b7 Preparing proposals\\n\\n\\u00b7 General business functions, including accounting (Quickbooks) and developing and implementing written policies and procedures\\n\\nWhile knowledge of healthcare is very much appreciated, the number one hiring criteria is that you be an experienced data-user and a fast-learning person who is able and willing to jump into ill-defined tasks and get them done \\u2013 well.\\n\\nSubmit a resume and a succinct cover letter. Within the cover letter, describe what will make you a great contributor to Teus Health\\u2019s growth. While you don\\u2019t have to have all the above skills, you need to sell your ability to deliver excellence. What excellence have you delivered in the past? The cover letter is also a writing sample, so write it without help from others.\\n\\nDr. Sawhney is committed to building a Newark, NJ-centered business. Therefore, preference will be given to candidates who live in Newark or nearby. You may be a student and if this work helps you satisfy degree requirements, Dr. Sawhney (an adjunct professor at NYU) will assist with the paperwork. Curricular practical training (CPT) sponsorship is possible for an exceptional candidate on an F-1 visa, but only if approval can be expedited.\\n\\nDr. Sawhney does not discriminate based on race, creed, color, national origin, ancestry, marital status, gender identity or expression, affectional or sexual orientation, or sex \\u2013 nor does she welcome those that do.\\n\\nJob Types: Full-time, Part-time, Temporary\\n\\nPay: $25.00 per hour\\n\\nSchedule:\\n\\nMonday to Friday\\nCOVID-19 considerations:\\nOnly Dr. Sawhney will be in the office with you with separate rooms and workspaces. She takes sanitation, mask, and handwashing protocols seriously upon any exit or entry of the building, and practices social distancing outside of work.\\n\\nWork Location:\\nOne location\\nTypical start time:\\n9AM\\nTypical end time:\\n5PM\\nThis Job Is Ideal for Someone Who Is:\\nDependable -- more reliable than spontaneous\\nAdaptable/flexible -- enjoys doing work that requires frequent shifts in direction\\nDetail-oriented -- would rather focus on the details of work than the bigger picture\\nAutonomous/Independent -- enjoys working with little direction\\nThis Company Describes Its Culture as:\\nDetail-oriented -- quality and precision-focused\\nOutcome-oriented -- results-focused with strong performance culture\\nStable -- traditional, stable, strong processes\\nCompany's website:\\nteushealth.com\\nWork Remotely:\\nNo\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sector\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"Non-Profit\",\n          \"Health Care\",\n          \"Restaurants, Bars & Food Services\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Quzx5tnjUtl"
      },
      "source": [
        "## Preprocesar la información"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8hlexmRjXIS"
      },
      "source": [
        "### Limpieza"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### convertir el df en dataframe\n"
      ],
      "metadata": {
        "id": "HdThE4iqtAEw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBSUDL-UP-W_"
      },
      "source": [
        "df = pd.DataFrame(df)\n",
        "\n",
        "# Limpiar el campo de estimación salarial\n",
        "df[\"Salary Estimate\"] = df[\"Salary Estimate\"].str.replace(r\" \\(Glassdoor est.\\)\", \"\", regex=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de normalización y conversión en variable numérica para la predicción\n",
        "df[\"Salary Estimate\"] = df[\"Salary Estimate\"].str.replace('K', '000')\n"
      ],
      "metadata": {
        "id": "LQYjlxloXAXI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# conversión de X en data_clean, sumando cada uno de los vectores\n",
        "data_clean= df['Job Title']+ \" \" + df['Job Description']+ \" \" + df['Sector']"
      ],
      "metadata": {
        "id": "M9-uOIQVLDYo"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"data_clean\"]= data_clean"
      ],
      "metadata": {
        "id": "yo9r-GP9te4h"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jiMaQsLWiTS"
      },
      "source": [
        "\n",
        "# Convertir rangos a valores numéricos y calcular el promedio\n",
        "df['min_salary'] = df['Salary Estimate'].str.split('-').str[0].str.replace('K', '').str.replace('$', '').astype(int) * 1000\n",
        "df['max_salary'] = df['Salary Estimate'].str.split('-').str[1].str.replace('K', '').str.replace('$', '').astype(int) * 1000\n",
        "df['avg_salary'] = (df['min_salary'] + df['max_salary']) / 2\n",
        "\n",
        "# Mostrar las primeras filas después de la conversión para verificar\n",
        "#print(\"Primeras filas después de la conversión:\", df[['Salary Estimate', 'min_salary', 'max_salary', 'avg_salary']].head())\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['avg_salary']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XynuwNp2xrs9",
        "outputId": "21fa7cef-b5a2-43cd-bc77-5bcb006fe621"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       51500000.0\n",
              "1       51500000.0\n",
              "2       51500000.0\n",
              "3       51500000.0\n",
              "4       51500000.0\n",
              "          ...     \n",
              "257    100000000.0\n",
              "258    100000000.0\n",
              "259    100000000.0\n",
              "260    100000000.0\n",
              "261    100000000.0\n",
              "Name: avg_salary, Length: 262, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# de las tres posibilidades, uitilizaré el promedio"
      ],
      "metadata": {
        "id": "0j79HJJIttTD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_labels = df[\"avg_salary\"].values\n",
        "# Convert to a pandas Series\n",
        "data_labels = pd.Series(data_labels)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "print(data_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot75zYH_yXhr",
        "outputId": "9b2c1fcc-c8d9-4a30-d8ff-80c5fe1e95ae"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      51500000.00\n",
            "1      51500000.00\n",
            "2      51500000.00\n",
            "3      51500000.00\n",
            "4      51500000.00\n",
            "          ...     \n",
            "257   100000000.00\n",
            "258   100000000.00\n",
            "259   100000000.00\n",
            "260   100000000.00\n",
            "261   100000000.00\n",
            "Length: 262, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eh7sIquja5t"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K59PriX4jgBV"
      },
      "source": [
        "Necesitaremos crear una capa BERT para tener acceso a los meta datos para el tokenizador (como el tamaño del vocabulario)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 1: Importar las bibliotecas necesarias\n",
        "Asegúrate de que tienes las bibliotecas necesarias importadas para ejecutar el código, como tensorflow_hub y bert-for-tf2:"
      ],
      "metadata": {
        "id": "BOz3ghSrza4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "#!pip install bert-for-tf2\n",
        "import bert\n"
      ],
      "metadata": {
        "id": "P05hnDYZzdnD"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 2: Cargar la Capa BERT desde TensorFlow Hub\n",
        "El código carga un modelo BERT (bert_en_uncased_L-12_H-768_A-12) como una capa de Keras de TensorFlow Hub, que no será entrenable (trainable=False). Esto es útil para tareas que solo requieren la representación de BERT sin ajustar sus pesos:"
      ],
      "metadata": {
        "id": "0r5NZ-10zkOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)"
      ],
      "metadata": {
        "id": "D47plSVYzdj7"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 3: Preparar el Tokenizer\n",
        "Se prepara el tokenizer utilizando la información del modelo cargado:\n",
        "\n",
        "Archivo de Vocabulario: vocab_file se obtiene directamente del modelo BERT cargado, que contiene el vocabulario usado durante el entrenamiento del modelo.\n",
        "Case Sensitivity: do_lower_case indica si el modelo fue entrenado con textos en minúsculas, lo cual es importante para saber cómo procesar el texto de entrada."
      ],
      "metadata": {
        "id": "GFj7rEYkzrbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "metadata": {
        "id": "hEMVFDvpzdhE"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 4: Utilizar el Tokenizer\n",
        "Una vez que tienes el tokenizer configurado, puedes usarlo para tokenizar tus textos. Por ejemplo:"
      ],
      "metadata": {
        "id": "aSNEkt9-zzqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Here is an example sentence to tokenize using BERT tokenizer.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PcXW-pSzdec",
        "outputId": "0d68549c-66a0-4e80-e763-7dcb86653bb5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['here', 'is', 'an', 'example', 'sentence', 'to', 'token', '##ize', 'using', 'bert', 'token', '##izer', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 5: Aplicar el Tokenizer\n",
        "Una vez que tienes la columna data_clean lista, puedes aplicar el tokenizer a cada entrada de esta columna. Usaremos una comprensión de lista para aplicar el tokenizer a cada fila del DataFrame."
      ],
      "metadata": {
        "id": "USPb_2OQ0mMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que 'tokenizer' ya está inicializado como mostraste anteriormente\n",
        "df['tokens'] = df['data_clean'].apply(lambda x: tokenizer.tokenize(x))"
      ],
      "metadata": {
        "id": "BzJI9sI5zdb2"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tokens']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4kGf3gC1eP3",
        "outputId": "2d800392-f2b1-4e7b-d9b8-115544de4815"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      [data, analyst, ,, center, on, immigration, an...\n",
              "1      [quality, data, analyst, overview, provides, a...\n",
              "2      [senior, data, analyst, ,, insights, &, analyt...\n",
              "3      [data, analyst, re, ##qui, ##sit, ##ion, numbe...\n",
              "4      [reporting, data, analyst, about, fan, ##due, ...\n",
              "                             ...                        \n",
              "257    [data, analyst, -, qc, nes, ##co, resource, is...\n",
              "258    [people, operations, &, data, analyst, job, de...\n",
              "259    [lead, data, analyst, (, product, ), a, bit, a...\n",
              "260    [data, analyst, -, iii, direct, client, requir...\n",
              "261    [sql, data, analyst, job, title, :, senior, sq...\n",
              "Name: tokens, Length: 262, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código añadirá una nueva columna al DataFrame llamada 'tokens', donde cada entrada es la lista de tokens resultantes de aplicar el tokenizer BERT a la cadena correspondiente en 'data_clean'.\n",
        "\n",
        "### Paso 6: Verificación\n",
        "Es una buena práctica verificar que el proceso ha funcionado como se esperaba. Puedes inspeccionar las primeras filas del DataFrame para asegurarte de que los tokens se han generado correctamente:"
      ],
      "metadata": {
        "id": "cbcgYDgq00oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['data_clean', 'tokens']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4QVRqqJzdYO",
        "outputId": "ff24d075-76fc-4171-ea79-6414e15f39f5"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          data_clean  \\\n",
            "0  Data Analyst, Center on Immigration and Justic...   \n",
            "1  Quality Data Analyst Overview\\n\\nProvides anal...   \n",
            "2  Senior Data Analyst, Insights & Analytics Team...   \n",
            "3  Data Analyst Requisition NumberRR-0001939\\nRem...   \n",
            "4  Reporting Data Analyst ABOUT FANDUEL GROUP\\n\\n...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [data, analyst, ,, center, on, immigration, an...  \n",
            "1  [quality, data, analyst, overview, provides, a...  \n",
            "2  [senior, data, analyst, ,, insights, &, analyt...  \n",
            "3  [data, analyst, re, ##qui, ##sit, ##ion, numbe...  \n",
            "4  [reporting, data, analyst, about, fan, ##due, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 7: tokenizar las palabras"
      ],
      "metadata": {
        "id": "hMmRG08G2gKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función encode_sentence que mencionas está diseñada para convertir un texto de entrada en una lista de identificadores numéricos (IDs) correspondientes a los tokens en el vocabulario de BERT. Este proceso es una parte crucial en la preparación de datos para modelos basados en BERT, ya que estos modelos no trabajan directamente con texto plano sino con tokens que son convertidos en IDs numéricos."
      ],
      "metadata": {
        "id": "KmfERaZX2aWV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LggMv7k7Z3Ij"
      },
      "source": [
        "def encode_sentence(sent):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 8: Conversión de Tokens a IDs\n",
        "Una vez que el texto ha sido dividido en tokens, el siguiente paso es convertir cada token en su correspondiente ID numérico:"
      ],
      "metadata": {
        "id": "GIZOXYBi2tjL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGfTo5uIa2is"
      },
      "source": [
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-4oGSu5jxUi"
      },
      "source": [
        "# Creación del data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLg0Z7QOj_YZ"
      },
      "source": [
        "Crearemos padded batches (por lo que rellenamos las frases para cada lote de forma independiente), de esta forma añadimos el mínimo número de tokens de padding posible. Para eso, ordenamos las frases por longitud, aplicamos padded_batches y luego las mezclamos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS_f6gWsLfLM"
      },
      "source": [
        "data_with_len = [[sent, data_labels[i], len(sent)]\n",
        "                 for i, sent in enumerate(data_inputs)]\n",
        "random.shuffle(data_with_len)\n",
        "data_with_len.sort(key=lambda x: x[2])\n",
        "sorted_all = [(sent_lab[0], sent_lab[1])\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 7]"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry0uJJg8lSQR"
      },
      "source": [
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
        "                                             output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzHAhlfTlrcj"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrPqJeYpmfcv"
      },
      "source": [
        "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10\n",
        "all_batched.shuffle(NB_BATCHES)\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
        "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU-YHkMVVqVY",
        "outputId": "02ccbc92-b247-4d67-cf0a-75cac6a005d8"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws9aABiFVtVW",
        "outputId": "c7e2a49f-1f69-4a5d-98a2-f7123d44029a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_SkipDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxONsFVHkFLU"
      },
      "source": [
        "# Fase 3: Construcción del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6DD3k3qPLDQ"
      },
      "source": [
        "#DCNN que hereda de tf.keras.Model de TensorFlow Keras API, diseñada específicamente para trabajar con datos textuales,\n",
        "#probablemente para tareas como clasificación de texto.\n",
        "class DCNN(tf.keras.Model):\n",
        "  #El método __init__ inicializa una nueva instancia de DCNN. Aquí es donde configuras todas las capas y parámetros que necesitará tu modelo:\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 emb_dim=128,\n",
        "                 nb_filters=50,\n",
        "                 FFN_units=512,\n",
        "                 nb_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"dcnn\"):\n",
        "        super(DCNN, self).__init__(name=name)\n",
        "\n",
        "        self.embedding = layers.Embedding(vocab_size,\n",
        "                                          emb_dim)\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
        "                                    kernel_size=2,\n",
        "                                    padding=\"valid\",\n",
        "                                    activation=\"relu\")\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
        "                                     kernel_size=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
        "                                      kernel_size=4,\n",
        "                                      padding=\"valid\",\n",
        "                                      activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "\n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                           activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x = self.embedding(inputs)\n",
        "        x_1 = self.bigram(x) # (batch_size, nb_filters, seq_len-1)\n",
        "        x_1 = self.pool(x_1) # (batch_size, nb_filters)\n",
        "        x_2 = self.trigram(x) # (batch_size, nb_filters, seq_len-2)\n",
        "        x_2 = self.pool(x_2) # (batch_size, nb_filters)\n",
        "        x_3 = self.fourgram(x) # (batch_size, nb_filters, seq_len-3)\n",
        "        x_3 = self.pool(x_3) # (batch_size, nb_filters)\n",
        "\n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
        "        merged = self.dense_1(merged)\n",
        "        merged = self.dropout(merged, training)\n",
        "        output = self.last_dense(merged)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSix1l4jkIxp"
      },
      "source": [
        "# Fase 4: Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhfUFvWEPOIf"
      },
      "source": [
        "VOCAB_SIZE = len(tokenizer.vocab)\n",
        "EMB_DIM = 200\n",
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "NB_EPOCHS = 5"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMtdiWmwv6rD"
      },
      "source": [
        "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
        "            emb_dim=EMB_DIM,\n",
        "            nb_filters=NB_FILTERS,\n",
        "            FFN_units=FFN_UNITS,\n",
        "            nb_classes=NB_CLASSES,\n",
        "            dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6apbd7FrwPYo"
      },
      "source": [
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78cceSGCw1XC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d391c6-a271-472e-d083-9fee1b9fe480"
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Último checkpoint restaurado!!\")"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Último checkpoint restaurado!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YIF5trzx7RA"
      },
      "source": [
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        ckpt_manager.save()\n",
        "        print(\"Checkpoint guardado en {}.\".format(checkpoint_path))"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrT8oWZzQNmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da7b0e94-df1b-4722-b60e-adee40207a5a"
      },
      "source": [
        "Dcnn.fit(train_dataset,\n",
        "         epochs=NB_EPOCHS,\n",
        "         callbacks=[MyCustomCallback()])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "      9/Unknown - 3s 109ms/step - loss: -11822012825600.0000 - accuracy: 0.0000e+00Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/.\n",
            "9/9 [==============================] - 4s 148ms/step - loss: -11822012825600.0000 - accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "8/9 [=========================>....] - ETA: 0s - loss: -14836605911040.0000 - accuracy: 0.0000e+00Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/.\n",
            "9/9 [==============================] - 1s 133ms/step - loss: -14944920666112.0000 - accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "8/9 [=========================>....] - ETA: 0s - loss: -18580684931072.0000 - accuracy: 0.0000e+00Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/.\n",
            "9/9 [==============================] - 0s 56ms/step - loss: -18704372858880.0000 - accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "8/9 [=========================>....] - ETA: 0s - loss: -22960207298560.0000 - accuracy: 0.0000e+00Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/.\n",
            "9/9 [==============================] - 1s 60ms/step - loss: -23100999598080.0000 - accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "8/9 [=========================>....] - ETA: 0s - loss: -28184238096384.0000 - accuracy: 0.0000e+00Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/.\n",
            "9/9 [==============================] - 0s 53ms/step - loss: -28347599945728.0000 - accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e2f5dd06b90>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IiDW919kQQK"
      },
      "source": [
        "# Fase 5: Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo: Si tienes 1000 ejemplos y un tamaño de lote de 32\n",
        "total_test_samples = 1000\n",
        "batch_size = 32\n",
        "\n",
        "steps_per_epoch = total_test_samples // batch_size\n",
        "if total_test_samples % batch_size != 0:\n",
        "    steps_per_epoch += 100  # Añadir un paso adicional si hay un remanente\n",
        "\n",
        "results = Dcnn.evaluate(test_dataset, steps=steps_per_epoch)\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jjJnCfKfEOf",
        "outputId": "52d49ccf-82d5-4917-9de2-356d944daa81"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 131 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r131/131 [==============================] - 0s 1ms/step\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU0FPb-Nv0YO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5658bf85-a908-44a9-b5d1-c24477442216"
      },
      "source": [
        "results"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17dbJ-bS_Izy"
      },
      "source": [
        "\n",
        "\n",
        "*   Training: 88.5%\n",
        "*   Testing: 84.6%\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-jrRvtl1xuk"
      },
      "source": [
        "def get_prediction(sentence):\n",
        "    tokens = encode_sentence(sentence)\n",
        "    inputs = tf.expand_dims(tokens, 0)\n",
        "\n",
        "    output = Dcnn(inputs, training=False)\n",
        "\n",
        "    sentiment = math.floor(output*2)\n",
        "\n",
        "    if sentiment == 0:\n",
        "        print(\"Salida del modelo: {}\\nSentimiento predicho: Negativo.\".format(\n",
        "            output))\n",
        "    elif sentiment == 1:\n",
        "        print(\"Salida del modelo: {}\\nSentimiento predicho: Positivo.\".format(\n",
        "            output))"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk8V2bdvwfCv"
      },
      "source": [
        "get_prediction(\"This movie was pretty interesting.\")"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIcwVVB7wFUM"
      },
      "source": [
        "get_prediction(\"I'd rather not do that again.\")"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qRnG7ojuNCBr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}