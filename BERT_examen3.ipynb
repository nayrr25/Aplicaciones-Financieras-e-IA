{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nayrr25/Aplicaciones-Financieras-e-IA/blob/main/BERT_examen3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JeFdEXRfrOyw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca6rYUxNi_MO"
      },
      "source": [
        "# Fase 1: Importar las dependencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76HfPILdC5lD"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1h4YVFfDd1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f262bf93-cb8c-492b-d620-9a18795d586a"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.10/dist-packages (0.14.9)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.10/dist-packages (from bert-for-tf2) (0.10.2)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.66.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMqTwu9jENrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85db6653-12fc-4433-818d-5f946ef12672"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0_xu0I3jFP9"
      },
      "source": [
        "# Fase 2: Pre procesado de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v60JTFKojIq5"
      },
      "source": [
        "## Carga de los ficheros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTtO7NkPjKUd"
      },
      "source": [
        "Importamos los ficheros desde nuestro Google Drive personal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRCxQui8Gqi_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c8b107-c2f2-4591-afe3-3b206dcf69ce"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6iT5nxDHLRz"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Asegúrate de usar la ruta correcta a tu archivo\n",
        "file_path = '/content/drive/MyDrive/AplicacionesFinancieras/DataAnalyst.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X= df[['Job Title', 'Job Description','Sector']]\n",
        "y= df[['Salary Estimate']]"
      ],
      "metadata": {
        "id": "v3xQ59OVI2ns"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKnCVewUIBkc"
      },
      "source": [],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWWUo_XVeqoG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "c4bf0e26-e490-4f1f-8180-1568538aa541"
      },
      "source": [
        "X.head(6)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           Job Title  \\\n",
              "0  Data Analyst, Center on Immigration and Justic...   \n",
              "1                               Quality Data Analyst   \n",
              "2  Senior Data Analyst, Insights & Analytics Team...   \n",
              "3                                       Data Analyst   \n",
              "4                             Reporting Data Analyst   \n",
              "5                                       Data Analyst   \n",
              "\n",
              "                                     Job Description  \\\n",
              "0  Are you eager to roll up your sleeves and harn...   \n",
              "1  Overview\\n\\nProvides analytical and technical ...   \n",
              "2  We’re looking for a Senior Data Analyst who ha...   \n",
              "3  Requisition NumberRR-0001939\\nRemote:Yes\\nWe c...   \n",
              "4  ABOUT FANDUEL GROUP\\n\\nFanDuel Group is a worl...   \n",
              "5  About Cubist\\nCubist Systematic Strategies is ...   \n",
              "\n",
              "                             Sector  \n",
              "0                        Non-Profit  \n",
              "1                       Health Care  \n",
              "2            Information Technology  \n",
              "3            Information Technology  \n",
              "4  Arts, Entertainment & Recreation  \n",
              "5                           Finance  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd3a6498-3959-43a6-9143-080d0615d4c4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Job Title</th>\n",
              "      <th>Job Description</th>\n",
              "      <th>Sector</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Data Analyst, Center on Immigration and Justic...</td>\n",
              "      <td>Are you eager to roll up your sleeves and harn...</td>\n",
              "      <td>Non-Profit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Quality Data Analyst</td>\n",
              "      <td>Overview\\n\\nProvides analytical and technical ...</td>\n",
              "      <td>Health Care</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Senior Data Analyst, Insights &amp; Analytics Team...</td>\n",
              "      <td>We’re looking for a Senior Data Analyst who ha...</td>\n",
              "      <td>Information Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>Requisition NumberRR-0001939\\nRemote:Yes\\nWe c...</td>\n",
              "      <td>Information Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reporting Data Analyst</td>\n",
              "      <td>ABOUT FANDUEL GROUP\\n\\nFanDuel Group is a worl...</td>\n",
              "      <td>Arts, Entertainment &amp; Recreation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>About Cubist\\nCubist Systematic Strategies is ...</td>\n",
              "      <td>Finance</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd3a6498-3959-43a6-9143-080d0615d4c4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dd3a6498-3959-43a6-9143-080d0615d4c4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dd3a6498-3959-43a6-9143-080d0615d4c4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f8c6f911-70da-4cb5-8626-18f4207500e6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f8c6f911-70da-4cb5-8626-18f4207500e6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f8c6f911-70da-4cb5-8626-18f4207500e6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X",
              "summary": "{\n  \"name\": \"X\",\n  \"rows\": 262,\n  \"fields\": [\n    {\n      \"column\": \"Job Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 169,\n        \"samples\": [\n          \"Big Data Programmer Analyst\",\n          \"Market Data Reporting Analyst\",\n          \"Data Analyst, Enterprise Analytics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Job Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 262,\n        \"samples\": [\n          \"Requisition no: 507136\\nWork type: Full Time\\nLocation: Medical Center\\nSchool/Department: Medicine\\nGrade: Grade 103\\nCategories: Research (Lab and Non-Lab)\\nJob Type: Officer of Administration\\nBargaining Unit: N/A\\nRegular/Temporary: Regular\\nEnd Date if Temporary: N/A\\nHours Per Week: 35\\nSalary Range: Commensurate with experience\\nPosition Summary\\n\\nWe are an innovative research entity in the Division of Cardiology looking for a highly organized, detail-oriented and enthusiastic candidate to support the patient-oriented research portfolio of the Center. The Data Analyst will conduct data management activities and analysis for clinical research projects within the Center. The Data Analyst will work in conjunction with the Principal Investigator, other study investigators, and staff to coordinate data management and analysis activities.\\n\\nResponsibilities\\n\\nResponsibilities include:\\nDevelopment of data collection and data entry systems\\nRunning regular data quality assurance checks\\nOrganizing and merging research data from multiple sources\\nPreparing summary statistics, tables and figures for reports, presentations and publications\\nAssisting with the interpretation and dissemination of findings in peer-reviewed journals\\nCoordinating with research staff to ensure the accuracy and efficiency of the data collection and data entry tools and processes\\nPerforming other responsibilities as assigned and requested\\nMinimum Qualifications\\nRequires a bachelor's degree or equivalent in education, training and experience, plus two years of related experience.\\nPreferred Qualifications\\nExperience with database management (including REDCAP) preferred.\\nExperience with FileMaker Pro.\\nOther Requirements\\nExperience working with large datasets, statistical analysis, and strong working knowledge of advanced SAS and SPSS (or other statistical/database software or other programming language) required.\\nWell-organized, proactive, extremely organized and detail-oriented with excellent time management skills.\\nAble to balance working independently and within a multidisciplinary team\\nExcellent interpersonal, and communication skills.\\nAble to multitask in a diverse and demanding environment with frequently shifting priorities.\\nAble to demonstrate flexibility in workload/work hours to meet critical deadlines.\\nAbility to prioritize multiple projects to ensure the completion of essential tasks.\\nMust successfully complete systems training requirements.\\nEqual Opportunity Employer / Disability / Veteran\\n\\nColumbia University is committed to the hiring of qualified local residents.\\n\\nApplications open: Mar 18 2020 Eastern Daylight Time Applications close:\\n\\nBack Apply Share\",\n          \"Job Description\\n\\nJob ID#:\\n2359\\n\\nJob Category:\\nBehavioral Health (Promesa)\\n\\nPosition Type:\\nFull Time\\n\\nDetails:\\n\\n\\nAcacia Network, the leading Latino integrated care nonprofit in the nation, offers the community, from children to seniors, a pathway to behavioral and primary healthcare, housing, and empowerment. We are visionary leaders transforming the triple aim of high quality, great experience at a lower cost. Acacia champions a collaborative environment to deliver vital health, housing and community building services, work we have been doing since 1969. By hiring talented individuals like you, we\\u2019ve been able to expand quickly, with offices in Albany, Buffalo, Syracuse, Orlando, Tennessee, Maryland and Puerto Rico.\\n\\nThe Data Analyst is responsible for entering information into the Nextgen EHR, importing data results for analysis, analyzing data trends, completing QA/QI to improve program performance and outcomes based on trends and will be a member of the integrated treatment team. Under supervision of the Program Director, the requirements listed below represent the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities as defined by the ADA to perform the essential functions of the job. The functions below outline the general responsibilities associated with this position. Each of these functions is considered critical to effective department operations and our Mission. It is expected that all staff in this position will have satisfactory attendance and will perform these functions as assigned in a competent, cooperative, and timely manner. The Job Description will be reviewed periodically for accuracy and/or adjusted according to business necessity.\\n\\n\\nKEY ESSENTIAL FUNCTIONS:\\nWill enter all required data into Nextgen and run queries in SQL at the back end\\nWill run daily, weekly, monthly, and quarterly reports as per program needs and performance indicators.\\nGenerates quality indicator reports from Nextgen database as requested and assigned by supervisor.\\nMaintains Data spreadsheet up-to-date by keeping all required data indicators as per program regulations by mapping from front end to back end and validating it.\\nCoordinates and maintains record of consumer participation in the program by conducting surveys, activities and integrating consumers to participate in activities as per program regulations.\\nGenerates and inputs discharge and re-entry information for consumers on the Nextgen database as per program requirements and assigned by supervisor.\\nParticipates in integrated team meetings, collects data information as per program requirements and maintain accuracy in data spreadsheet.\\nComplete quality improvement projects based on data trends.\\nAct as curator of new datasets, documenting and performing quality checks.\\nCompletes all required training as per program regulations for entering consumer data and as assigned by supervisor.\\nCompletes office duties as needed/required by Supervisor.\\nCompletes additional tasks as assigned by Supervisor.\\n\\nREQUIREMENTS:\\nCollege degree preferred\\nKnowledge of the Nextgen database and proficiency in SQL\\nExperience or desire to work with people who have a mental illness\\nPositive attitude and professional demeanor\\nA self starter\\nAbility to complete work independently as well as in collaboration with team members\\nMust be team oriented with a willingness to be flexible and helpful.\\nExcellent computer skills including, Microsoft Windows, Excel, PowerPoint, and electronic communications tools: internet and email\\nAbility to communicate effectively orally and in writing\\nHighly organized, motivated self-starter\\nExcellent time management skills.\\nAcacia Network is an equal opportunity employer*\\n\\nJob Requirements\\n\\n\\nAlready have an account? Log in here\",\n          \"Data Analyst Assistant\\n\\nCalling data-loving entrepreneurs! Teus Health (www.teushealth.com), a healthcare policy and analytics firm, is growing and the owner, Dr. Tia Goss Sawhney, needs to grow her team. The work may be part- or full-time. Covid conditions permitting, the work is on-site at Teus Health\\u2019s downtown Newark office.\\n\\nDr. Sawhney needs your skills for:\\n\\n\\u00b7 Healthcare data analysis in SQL, Python, and/or other data analytic tools\\n\\n\\u00b7 Building and maintaining Excel models, requiring advanced Excel and VBA skills\\n\\n\\u00b7 Supporting and improving the firm\\u2019s IT environment and the security thereof\\n\\n\\u00b7 Preparing proposals\\n\\n\\u00b7 General business functions, including accounting (Quickbooks) and developing and implementing written policies and procedures\\n\\nWhile knowledge of healthcare is very much appreciated, the number one hiring criteria is that you be an experienced data-user and a fast-learning person who is able and willing to jump into ill-defined tasks and get them done \\u2013 well.\\n\\nSubmit a resume and a succinct cover letter. Within the cover letter, describe what will make you a great contributor to Teus Health\\u2019s growth. While you don\\u2019t have to have all the above skills, you need to sell your ability to deliver excellence. What excellence have you delivered in the past? The cover letter is also a writing sample, so write it without help from others.\\n\\nDr. Sawhney is committed to building a Newark, NJ-centered business. Therefore, preference will be given to candidates who live in Newark or nearby. You may be a student and if this work helps you satisfy degree requirements, Dr. Sawhney (an adjunct professor at NYU) will assist with the paperwork. Curricular practical training (CPT) sponsorship is possible for an exceptional candidate on an F-1 visa, but only if approval can be expedited.\\n\\nDr. Sawhney does not discriminate based on race, creed, color, national origin, ancestry, marital status, gender identity or expression, affectional or sexual orientation, or sex \\u2013 nor does she welcome those that do.\\n\\nJob Types: Full-time, Part-time, Temporary\\n\\nPay: $25.00 per hour\\n\\nSchedule:\\n\\nMonday to Friday\\nCOVID-19 considerations:\\nOnly Dr. Sawhney will be in the office with you with separate rooms and workspaces. She takes sanitation, mask, and handwashing protocols seriously upon any exit or entry of the building, and practices social distancing outside of work.\\n\\nWork Location:\\nOne location\\nTypical start time:\\n9AM\\nTypical end time:\\n5PM\\nThis Job Is Ideal for Someone Who Is:\\nDependable -- more reliable than spontaneous\\nAdaptable/flexible -- enjoys doing work that requires frequent shifts in direction\\nDetail-oriented -- would rather focus on the details of work than the bigger picture\\nAutonomous/Independent -- enjoys working with little direction\\nThis Company Describes Its Culture as:\\nDetail-oriented -- quality and precision-focused\\nOutcome-oriented -- results-focused with strong performance culture\\nStable -- traditional, stable, strong processes\\nCompany's website:\\nteushealth.com\\nWork Remotely:\\nNo\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sector\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"Non-Profit\",\n          \"Health Care\",\n          \"Restaurants, Bars & Food Services\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Quzx5tnjUtl"
      },
      "source": [
        "## Preprocesar la información"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8hlexmRjXIS"
      },
      "source": [
        "### Limpieza"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### convertir el df en dataframe\n"
      ],
      "metadata": {
        "id": "HdThE4iqtAEw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBSUDL-UP-W_"
      },
      "source": [
        "df = pd.DataFrame(df)\n",
        "\n",
        "# Limpiar el campo de estimación salarial\n",
        "df[\"Salary Estimate\"] = df[\"Salary Estimate\"].str.replace(r\" \\(Glassdoor est.\\)\", \"\", regex=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de normalización y conversión en variable numérica para la predicción\n",
        "df[\"Salary Estimate\"] = df[\"Salary Estimate\"].str.replace('K', '000')\n"
      ],
      "metadata": {
        "id": "LQYjlxloXAXI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# conversión de X en data_clean, sumando cada uno de los vectores\n",
        "data_clean= df['Job Title']+ \" \" + df['Job Description']+ \" \" + df['Sector']"
      ],
      "metadata": {
        "id": "M9-uOIQVLDYo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"data_clean\"]= data_clean"
      ],
      "metadata": {
        "id": "yo9r-GP9te4h"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jiMaQsLWiTS"
      },
      "source": [
        "\n",
        "# Convertir rangos a valores numéricos y calcular el promedio\n",
        "df['min_salary'] = df['Salary Estimate'].str.split('-').str[0].str.replace('K', '').str.replace('$', '').astype(int) * 1000\n",
        "df['max_salary'] = df['Salary Estimate'].str.split('-').str[1].str.replace('K', '').str.replace('$', '').astype(int) * 1000\n",
        "df['avg_salary'] = (df['min_salary'] + df['max_salary']) / 2\n",
        "\n",
        "# Mostrar las primeras filas después de la conversión para verificar\n",
        "#print(\"Primeras filas después de la conversión:\", df[['Salary Estimate', 'min_salary', 'max_salary', 'avg_salary']].head())\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['avg_salary']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XynuwNp2xrs9",
        "outputId": "6857a22d-9acb-4ac5-ac9c-4980efc6b023"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       51500000.0\n",
              "1       51500000.0\n",
              "2       51500000.0\n",
              "3       51500000.0\n",
              "4       51500000.0\n",
              "          ...     \n",
              "257    100000000.0\n",
              "258    100000000.0\n",
              "259    100000000.0\n",
              "260    100000000.0\n",
              "261    100000000.0\n",
              "Name: avg_salary, Length: 262, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# de las tres posibilidades, uitilizaré el promedio"
      ],
      "metadata": {
        "id": "0j79HJJIttTD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_labels = df[\"avg_salary\"].values\n",
        "# Convert to a pandas Series\n",
        "data_labels = pd.Series(data_labels)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "print(data_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot75zYH_yXhr",
        "outputId": "2ae588aa-3b77-4697-a562-d17ecbba8454"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      51500000.00\n",
            "1      51500000.00\n",
            "2      51500000.00\n",
            "3      51500000.00\n",
            "4      51500000.00\n",
            "          ...     \n",
            "257   100000000.00\n",
            "258   100000000.00\n",
            "259   100000000.00\n",
            "260   100000000.00\n",
            "261   100000000.00\n",
            "Length: 262, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eh7sIquja5t"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K59PriX4jgBV"
      },
      "source": [
        "Necesitaremos crear una capa BERT para tener acceso a los meta datos para el tokenizador (como el tamaño del vocabulario)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 1: Importar las bibliotecas necesarias\n",
        "Asegúrate de que tienes las bibliotecas necesarias importadas para ejecutar el código, como tensorflow_hub y bert-for-tf2:"
      ],
      "metadata": {
        "id": "BOz3ghSrza4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "#!pip install bert-for-tf2\n",
        "import bert\n"
      ],
      "metadata": {
        "id": "P05hnDYZzdnD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 2: Cargar la Capa BERT desde TensorFlow Hub\n",
        "El código carga un modelo BERT (bert_en_uncased_L-12_H-768_A-12) como una capa de Keras de TensorFlow Hub, que no será entrenable (trainable=False). Esto es útil para tareas que solo requieren la representación de BERT sin ajustar sus pesos:"
      ],
      "metadata": {
        "id": "0r5NZ-10zkOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)"
      ],
      "metadata": {
        "id": "D47plSVYzdj7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 3: Preparar el Tokenizer\n",
        "Se prepara el tokenizer utilizando la información del modelo cargado:\n",
        "\n",
        "Archivo de Vocabulario: vocab_file se obtiene directamente del modelo BERT cargado, que contiene el vocabulario usado durante el entrenamiento del modelo.\n",
        "Case Sensitivity: do_lower_case indica si el modelo fue entrenado con textos en minúsculas, lo cual es importante para saber cómo procesar el texto de entrada."
      ],
      "metadata": {
        "id": "GFj7rEYkzrbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "metadata": {
        "id": "hEMVFDvpzdhE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 4: Utilizar el Tokenizer\n",
        "Una vez que tienes el tokenizer configurado, puedes usarlo para tokenizar tus textos. Por ejemplo:"
      ],
      "metadata": {
        "id": "aSNEkt9-zzqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Here is an example sentence to tokenize using BERT tokenizer.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PcXW-pSzdec",
        "outputId": "acbd2420-17bb-4f9d-b8aa-56e2b8e22d53"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['here', 'is', 'an', 'example', 'sentence', 'to', 'token', '##ize', 'using', 'bert', 'token', '##izer', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 5: Aplicar el Tokenizer\n",
        "Una vez que tienes la columna data_clean lista, puedes aplicar el tokenizer a cada entrada de esta columna. Usaremos una comprensión de lista para aplicar el tokenizer a cada fila del DataFrame."
      ],
      "metadata": {
        "id": "USPb_2OQ0mMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que 'tokenizer' ya está inicializado como mostraste anteriormente\n",
        "df['tokens'] = df['data_clean'].apply(lambda x: tokenizer.tokenize(x))"
      ],
      "metadata": {
        "id": "BzJI9sI5zdb2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tokens']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4kGf3gC1eP3",
        "outputId": "7498b05e-7c4f-48df-8470-f3a5a590a628"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      [data, analyst, ,, center, on, immigration, an...\n",
              "1      [quality, data, analyst, overview, provides, a...\n",
              "2      [senior, data, analyst, ,, insights, &, analyt...\n",
              "3      [data, analyst, re, ##qui, ##sit, ##ion, numbe...\n",
              "4      [reporting, data, analyst, about, fan, ##due, ...\n",
              "                             ...                        \n",
              "257    [data, analyst, -, qc, nes, ##co, resource, is...\n",
              "258    [people, operations, &, data, analyst, job, de...\n",
              "259    [lead, data, analyst, (, product, ), a, bit, a...\n",
              "260    [data, analyst, -, iii, direct, client, requir...\n",
              "261    [sql, data, analyst, job, title, :, senior, sq...\n",
              "Name: tokens, Length: 262, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código añadirá una nueva columna al DataFrame llamada 'tokens', donde cada entrada es la lista de tokens resultantes de aplicar el tokenizer BERT a la cadena correspondiente en 'data_clean'.\n",
        "\n",
        "### Paso 6: Verificación\n",
        "Es una buena práctica verificar que el proceso ha funcionado como se esperaba. Puedes inspeccionar las primeras filas del DataFrame para asegurarte de que los tokens se han generado correctamente:"
      ],
      "metadata": {
        "id": "cbcgYDgq00oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['data_clean', 'tokens']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4QVRqqJzdYO",
        "outputId": "22df5299-e460-47fe-d784-ba82dd925a35"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          data_clean  \\\n",
            "0  Data Analyst, Center on Immigration and Justic...   \n",
            "1  Quality Data Analyst Overview\\n\\nProvides anal...   \n",
            "2  Senior Data Analyst, Insights & Analytics Team...   \n",
            "3  Data Analyst Requisition NumberRR-0001939\\nRem...   \n",
            "4  Reporting Data Analyst ABOUT FANDUEL GROUP\\n\\n...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [data, analyst, ,, center, on, immigration, an...  \n",
            "1  [quality, data, analyst, overview, provides, a...  \n",
            "2  [senior, data, analyst, ,, insights, &, analyt...  \n",
            "3  [data, analyst, re, ##qui, ##sit, ##ion, numbe...  \n",
            "4  [reporting, data, analyst, about, fan, ##due, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 7: tokenizar las palabras"
      ],
      "metadata": {
        "id": "hMmRG08G2gKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función encode_sentence que mencionas está diseñada para convertir un texto de entrada en una lista de identificadores numéricos (IDs) correspondientes a los tokens en el vocabulario de BERT. Este proceso es una parte crucial en la preparación de datos para modelos basados en BERT, ya que estos modelos no trabajan directamente con texto plano sino con tokens que son convertidos en IDs numéricos."
      ],
      "metadata": {
        "id": "KmfERaZX2aWV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LggMv7k7Z3Ij"
      },
      "source": [
        "def encode_sentence(sent):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 8: Conversión de Tokens a IDs\n",
        "Una vez que el texto ha sido dividido en tokens, el siguiente paso es convertir cada token en su correspondiente ID numérico:"
      ],
      "metadata": {
        "id": "GIZOXYBi2tjL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGfTo5uIa2is"
      },
      "source": [
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación de los Componentes\n",
        "input_ids: Son los índices de los tokens en el vocabulario de BERT. Cada token del texto es convertido en un índice numérico que representa ese token en el vocabulario entrenado del modelo.\n",
        "\n",
        "attention_mask: Es un arreglo que indica a modelo qué tokens deben ser atendidos y cuáles no. Por lo general, los tokens reales tienen una máscara de 1, y los tokens de relleno (padding) tienen una máscara de 0.\n",
        "\n",
        "token_type_ids: Esta característica se utiliza principalmente para tareas que involucran dos textos distintos (como preguntas y respuestas). Indica a qué secuencia pertenece cada token. Para la mayoría de las aplicaciones de un solo texto, este campo no es necesario y puede estar ausente si el tokenizer no lo produce.\n",
        "\n",
        "Opciones Adicionales\n",
        "TensorFlow o PyTorch: El argumento return_tensors='pt' que usaste indica que los tensores deben ser devueltos como tensores de PyTorch. Si estás trabajando con TensorFlow, deberías cambiarlo a return_tensors='tf'.\n",
        "Ejemplo Completo de Tokenización\n",
        "Aquí te dejo un ejemplo completo que muestra cómo preparar los datos y tokenizarlos, asumiendo que data_clean es una lista de textos:"
      ],
      "metadata": {
        "id": "dj1jrr9Dtbd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_clean"
      ],
      "metadata": {
        "id": "sKyDQNrwtnha",
        "outputId": "e6aada4d-db9f-46c5-f386-db858db0ef9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      Data Analyst, Center on Immigration and Justic...\n",
              "1      Quality Data Analyst Overview\\n\\nProvides anal...\n",
              "2      Senior Data Analyst, Insights & Analytics Team...\n",
              "3      Data Analyst Requisition NumberRR-0001939\\nRem...\n",
              "4      Reporting Data Analyst ABOUT FANDUEL GROUP\\n\\n...\n",
              "                             ...                        \n",
              "257    Data Analyst - QC Nesco Resource is seeking a ...\n",
              "258    People Operations & Data Analyst JOB DESCRIPTI...\n",
              "259    Lead Data Analyst (Product) A BIT ABOUT OUR DA...\n",
              "260    Data Analyst - III Direct Client Requirement\\n...\n",
              "261    SQL Data Analyst Job Title :Senior SQL Data An...\n",
              "Length: 262, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Asumiendo que data_clean es una serie de pandas\n",
        "texts = data_clean.tolist()\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Cargar el tokenizer de BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenizar los textos\n",
        "encoded_outputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Acceso a los componentes del resultado tokenizado\n",
        "input_ids = encoded_outputs['input_ids']\n",
        "attention_masks = encoded_outputs['attention_mask']\n",
        "token_type_ids = encoded_outputs.get('token_type_ids')  # Puede ser None\n",
        "\n",
        "# Verificación opcional: imprimir los primeros input_ids para verificar\n",
        "print(\"Input IDs example:\", input_ids[0])\n",
        "print(\"Attention Mask example:\", attention_masks[0])\n",
        "if token_type_ids is not None:\n",
        "    print(\"Token Type IDs example:\", token_type_ids[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "lnUS3PrAtZil",
        "outputId": "5adfb891-e625-4c14-af71-08eff84017fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs example: tensor([  101,  2951, 12941,  1010,  2415,  2006,  7521,  1998,  3425,  1006,\n",
            "        25022,  3501,  1007,  2024,  2017,  9461,  2000,  4897,  2039,  2115,\n",
            "        15114,  1998, 17445,  2951,  2000,  3298,  3343,  2689,  1029,  2079,\n",
            "         2017,  5959,  9033, 26169,  2083,  3375,  2951, 13462,  2015,  2000,\n",
            "         5665, 12717, 12556, 12878,  1998, 20062,  1029,  2079,  2017,  2156,\n",
            "         4426,  2551,  2005,  1037,  5300,  1011,  5533,  3029,  2007,  1037,\n",
            "         4432,  2000, 11147,  1996,  2087,  7827, 21321,  2015,  1997,  2256,\n",
            "         2154,  1029,  2057,  2024,  2559,  2000, 10887,  1037,  4408,  1010,\n",
            "         2524,  1011,  2551,  1010,  1998,  5541,  3265,  2007,  2844,  2951,\n",
            "         2968,  4813,  1998,  1037,  7645,  8426,  2000, 11560,  1005,  1055,\n",
            "         2916,  1012,  1996,  2951, 12941,  2097,  6509,  2007,  4106,  1998,\n",
            "         7316,  3791,  2005, 12297,  2015,  2415,  2006,  7521,  1998,  3425,\n",
            "         1006, 25022,  3501,  1007,  1010,  2551,  2408,  2049,  2783,  3934,\n",
            "         1998,  2925, 12297, 11107,  1012,  2040,  2057,  2024,  1024,  2631,\n",
            "         1999,  3777,  1010,  1996, 12297,  2820,  2003,  2019,  2981,  1010,\n",
            "         2512,  1011, 14254,  1010, 14495,  3029,  2008, 13585, 11532,  1999,\n",
            "         2470,  1010,  4087,  5375,  1010,  1998, 10467,  3934,  2000,  6509,\n",
            "         4177,  1999,  2231,  1998,  2942,  2554, 11628,  3425,  3343,  1998,\n",
            "         3218,  1010,  1998,  5335,  1996,  3001,  2111, 11160,  2006,  2005,\n",
            "         3425,  1998,  3808,  1012,  2057,  2817,  3471,  2008, 17727, 14728,\n",
            "         2529, 13372,  1998,  3425,  1012,  2057,  4405,  7300,  2008,  2024,\n",
            "         2012,  2320, 10938,  8082,  1998,  9353,  4048, 13331,  3468,  1012,\n",
            "         2057,  8526,  7578,  4279,  1999,  6727,  5981,  1012,  1998,  2057,\n",
            "        17445,  1996,  2373,  1997,  3350,  2000,  3298,  4621,  3343,  1998,\n",
            "         3218,  2054,  2020,  2725,  1024,  2057,  2024,  5094,  2000,  3857,\n",
            "         1037,  2929, 22591,  3070,  2231,  4177,  1010, 13010,  1010,  1998,\n",
            "         1996,  7521,  3423,  2578,  2451, 18790, 18117,  5415,  3423,  6630,\n",
            "         2005,  7489,  5307, 23702,  1012,  1999,  1996,  2227,  1997,  3706,\n",
            "         1011,  2039,  7521,  7285,  1010,  8817,  1997,  2512,  1011,  4480,\n",
            "         2024,  2012,  3891,  1997,  3668, 12345,  1998,  4568,  8745,  2013,\n",
            "         2037,  2945,  1998,  4279,  1012, 12297,  2015,  2415,  2006,  7521,\n",
            "         1998,  3425,  1006, 25022,  3501,  1007,  5826,  2007,  2231,  1010,\n",
            "         2512,  1011,  5618,  5826,  1010,  1998,  4279,  2000,  5335,  2231,\n",
            "         3001,  2008,  7461,  7489,  1998,  2037,  2945,  1012, 25022,  3501,\n",
            "        21497,  2015,  2195,  9053,  3423,  2578,  3454,  2005,  7489,  5307,\n",
            "        23702,  1010, 11791,  1998, 22164,  4405,  3454,  1010,  3640,  4087,\n",
            "         5375,  1010,  1998, 17976,  2981,  2470,  1998,  9312,  1012,  2008,\n",
            "         2015,  2073,  2017,  2272,  1999,  1024,  1996,  2951, 12941,  2097,\n",
            "         2490,  1996,  6401,  2565, 12644,  4073,  2083,  3180,  8822,  1998,\n",
            "         7316,  1997,  2976,  2231,  1998,  4942,  8663,  6494, 16761,  2951,\n",
            "         1012, 25022,  3501,  9020,  2195, 16350, 17881,  2008,  2448,  2006,\n",
            "        22091,  2015,  1998, 25222, 22071,  1998,  3594, 29296,  1010,  1054,\n",
            "         1010,  1998, 18750,  2000,  6133,  2951,  1012,  2023,  2003,  2019,\n",
            "         4495,  2000,  2393,  4338,  2019,  9525,  2120,  2470,  1998,  3343,\n",
            "        11376,  2004,  2112,  1997,  1037,  4056,  2136,  1997,  8519,  2551,\n",
            "         2000,  5335,  3229,  2000,  3425,  2005,  2512,  1011,  4480,  1012,\n",
            "        12297, 11014,  2000, 10887,  1037,  2951, 12941,  2000,  2147,  2006,\n",
            "         2536,  2951,  2968,  3934,  2007,  2049,  2415,  2006,  7521,  1998,\n",
            "         3425,  1006, 25022,  3501,  1007,  1012,  1999,  5792,  2007,  2060,\n",
            "         2951, 18288,  1010,  2023,  2597,  2097,  9125,  2147,  2408,  2195,\n",
            "         3934,  1010,  2107,  2004,  1996, 14477, 21408,  8737,  7088,  2098,\n",
            "         2336,   102])\n",
            "Attention Mask example: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n",
            "Token Type IDs example: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salaries= df[\"avg_salary\"]"
      ],
      "metadata": {
        "id": "oBf7IxMAcK8Z"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Crear un Modelo de Predicción con BERT\n",
        "Usaremos PyTorch como framework para construir un modelo de regresión que utilice BERT para procesar los textos y una capa lineal para predecir el salario. Primero, carga el modelo BERT preentrenado y añade una capa lineal para la predicción del salario:"
      ],
      "metadata": {
        "id": "m-3qrnS-vH2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class SalaryPredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SalaryPredictor, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.regressor = nn.Linear(768, 1)  # 768 es la dimensión de los embeddings de BERT\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output  # Salida agregada (CLS token)\n",
        "        salary = self.regressor(pooled_output)\n",
        "        return salary\n",
        "\n",
        "# Inicializa el modelo\n",
        "model = SalaryPredictor()\n"
      ],
      "metadata": {
        "id": "XOh8fd-XcK5R"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 3: Preparar el DataLoader\n",
        "Para entrenar el modelo, necesitas un DataLoader que combine los input_ids, attention_masks y los salarios. Suponiendo que ya has tokenizado data_clean y tienes encoded_outputs:"
      ],
      "metadata": {
        "id": "EylvsFYRvOWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convertir los salarios a tensores de PyTorch\n",
        "salary_tensors = torch.tensor(salaries.values, dtype=torch.float)\n",
        "\n",
        "# Crear un TensorDataset\n",
        "dataset = TensorDataset(encoded_outputs['input_ids'], encoded_outputs['attention_mask'], salary_tensors)\n",
        "\n",
        "# Crear un DataLoader\n",
        "data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
      ],
      "metadata": {
        "id": "6GOvuR_ZvMFy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 4: Configurar el Entrenamiento\n",
        "Define el optimizador y la función de pérdida, y procede a entrenar el modelo:"
      ],
      "metadata": {
        "id": "ltdDZt0_vRg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.optim import Adam\n",
        "\n",
        "# optimizer = Adam(model.parameters(), lr=2e-5)\n",
        "# criterion = nn.MSELoss()\n",
        "\n",
        "# # Ciclo de entrenamiento\n",
        "# model.train()\n",
        "# for epoch in range(4):  # Número de epochs\n",
        "#     for batch in data_loader:\n",
        "#         input_ids, attention_mask, salaries = batch\n",
        "#         optimizer.zero_grad()\n",
        "#         predictions = model(input_ids, attention_mask)\n",
        "#         loss = criterion(predictions.squeeze(), salaries)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#     print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "id": "kMFgcRz9vURx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 5: Evaluación del Modelo\n",
        "Después de entrenar el modelo, evalúalo usando un conjunto de datos de prueba para verificar su rendimiento en la predicción de salarios:"
      ],
      "metadata": {
        "id": "2BmOjgfkvW3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Asegúrate de cambiar al modo de evaluación\n",
        "# model.eval()\n",
        "# # Implementa la evaluación aquí similar al bucle de entrenamiento, pero sin retropropagación\n"
      ],
      "metadata": {
        "id": "VZxM3PIXvZG6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este enfoque te proporciona una estructura completa desde la preparación de datos hasta el entrenamiento de un modelo que usa BERT para tareas de regresión, como predecir salarios basados en descripciones de trabajos. Ajusta los parámetros y la arquitectura según sea necesario para adaptarlos a tus necesidades específicas."
      ],
      "metadata": {
        "id": "7tAQQvpyvbV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 4. 1. Optimización del Tamaño del Batch\n",
        "Debido a las limitaciones de memoria y velocidad de la CPU, es posible que necesites ajustar el tamaño del batch para ser más pequeño. Esto reducirá la cantidad de memoria requerida por batch y evitará que el sistema se quede sin memoria durante el entrenamiento o la inferencia:"
      ],
      "metadata": {
        "id": "gz9n12ZC1tco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "l8FunngH_hnW",
        "outputId": "7bdecffd-5fbf-40be-ba67-c7cefa085d0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataset.TensorDataset at 0x7aa916e35e10>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)  # Reducir el tamaño del batch si es necesario\n"
      ],
      "metadata": {
        "id": "ooSXnMHgw6NX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 5. Gestión de Memoria\n",
        "Asegúrate de gestionar bien la memoria:\n",
        "\n",
        "Evita cargar grandes cantidades de datos en memoria a la vez.\n",
        "Utiliza generadores o carga de datos por lotes cuando sea posible.\n",
        "##. 4. 6. Uso Eficiente del DataLoader\n",
        "Al utilizar el DataLoader, asegúrate de no sobrecargar la CPU. Puedes ajustar el número de workers a un número bajo para evitar el uso excesivo de múltiples núcleos:"
      ],
      "metadata": {
        "id": "3vf9jcgD2Seq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=1)  # Ajustar num_workers según la CPU"
      ],
      "metadata": {
        "id": "qzpizeZa2Yvt"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 2. Tiempo de Procesamiento\n",
        "Prepárate para tiempos de procesamiento más largos. En una CPU, las operaciones no pueden ser paralelizadas al mismo nivel que en una GPU, por lo que cada paso de entrenamiento o inferencia puede tomar más tiempo.\n",
        "\n",
        "## 4. 3. Uso de Modelos Pre-Optimizados\n",
        "Considera usar versiones de BERT que están optimizadas para ser más eficientes. Por ejemplo, distilBERT es una versión más pequeña y más rápida de BERT que retiene la mayoría de su rendimiento:"
      ],
      "metadata": {
        "id": "7kI38Z5e14S4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n"
      ],
      "metadata": {
        "id": "Gx0L1mPp136u"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Uso de Modelos Cuantificados\n",
        "La cuantificación reduce la precisión de los cálculos (de floating point a integers), lo que puede disminuir el uso de memoria y acelerar la inferencia. PyTorch ofrece soporte para cuantificación:"
      ],
      "metadata": {
        "id": "TVibafSF2Knz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de cuantificación dinámica para acelerar la inferencia\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model.eval()\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n"
      ],
      "metadata": {
        "id": "vGPCh5XE2FCi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo NUEVA CONFIGURACION\n",
        "Paso 1: Preparar los Datos\n",
        "Supongamos que ya tienes los datos tokenizados (input_ids, attention_mask) y una variable adicional como salaries.\n",
        "\n",
        "Paso 2: Modificar la Arquitectura del Modelo\n",
        "Para combinar estos datos, necesitas un modelo que no solo tome la salida de BERT, sino que también pueda integrar otras entradas. Aquí hay un ejemplo de cómo puedes estructurar este modelo:"
      ],
      "metadata": {
        "id": "Xapiom749Dc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class BertWithAdditionalFeatures(nn.Module):\n",
        "    def __init__(self, num_additional_features):\n",
        "        super(BertWithAdditionalFeatures, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.regressor = nn.Linear(768 + num_additional_features, 1)  # Asume 768 dimensiones de BERT + tus características adicionales\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, additional_features):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        # Concatenar la salida de BERT con las características adicionales\n",
        "        combined_features = torch.cat((pooled_output, additional_features), 1)\n",
        "        return self.regressor(combined_features)\n",
        "\n",
        "# Inicializa el modelo con el número de características adicionales, ej: 1 si es solo el salario\n",
        "model = BertWithAdditionalFeatures(num_additional_features=1)\n"
      ],
      "metadata": {
        "id": "sKE26-hO2YkC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 3: Preparar las Características Adicionales\n",
        "Asegúrate de que las características adicionales estén en el formato adecuado, por ejemplo, un tensor de PyTorch:"
      ],
      "metadata": {
        "id": "AdzXaHHV9Pq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Asegúrate de que los salarios son un tensor con la forma correcta\n",
        "additional_features = torch.tensor(salaries.values, dtype=torch.float32).view(-1, 1)  # Redimensiona si es necesario\n"
      ],
      "metadata": {
        "id": "QsqVePhq2gDX"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 4: Crear TensorDataset y DataLoader\n",
        "Cuando crees el TensorDataset y DataLoader, incluye estas características adicionales:"
      ],
      "metadata": {
        "id": "APJHi0L59T-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Suponiendo que encoded_outputs ya están definidos como antes\n",
        "input_ids = encoded_outputs['input_ids']\n",
        "attention_mask = encoded_outputs['attention_mask']\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_mask, additional_features, salary_tensors)\n",
        "data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
      ],
      "metadata": {
        "id": "XQywFwD02f__"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 5: Entrenamiento del Modelo\n",
        "Cuando entrenes el modelo, asegúrate de pasar todas las entradas necesarias:"
      ],
      "metadata": {
        "id": "eyfklAms9dRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import BertModel\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "criterion = nn.MSELoss()\n",
        "num_epochs = 1  # Puedes ajustar esto según tus necesidades y recursos\n",
        "\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in data_loader:\n",
        "        input_ids, attention_mask, additional_features, salaries = batch\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(input_ids, attention_mask, additional_features)\n",
        "        loss = criterion(predictions.squeeze(), salaries)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "id": "5lp4hJqg2f8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NhoAOLls9e6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHywUiEV9e3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgD1PiXY9ez9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l4Msighn9ew1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}